{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import keras\n",
    "# import cv2\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/projectnb/dl-course/jxtang/EC500_proj/train/\"\n",
    "test_path = \"/projectnb/dl-course/jxtang/EC500_proj/test/\"\n",
    "label_path = '/projectnb/dl-course/jxtang/EC500_proj/train.csv'\n",
    "train_files = listdir(train_path)\n",
    "test_files = listdir(test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv(label_path)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dict = {\n",
    "    0:  \"Nucleoplasm\",  \n",
    "    1:  \"Nuclear membrane\",   \n",
    "    2:  \"Nucleoli\",   \n",
    "    3:  \"Nucleoli fibrillar center\",   \n",
    "    4:  \"Nuclear speckles\",\n",
    "    5:  \"Nuclear bodies\",   \n",
    "    6:  \"Endoplasmic reticulum\",   \n",
    "    7:  \"Golgi apparatus\",   \n",
    "    8:  \"Peroxisomes\",   \n",
    "    9:  \"Endosomes\",   \n",
    "    10:  \"Lysosomes\",   \n",
    "    11:  \"Intermediate filaments\",   \n",
    "    12:  \"Actin filaments\",   \n",
    "    13:  \"Focal adhesion sites\",   \n",
    "    14:  \"Microtubules\",   \n",
    "    15:  \"Microtubule ends\",   \n",
    "    16:  \"Cytokinetic bridge\",   \n",
    "    17:  \"Mitotic spindle\",   \n",
    "    18:  \"Microtubule organizing center\",   \n",
    "    19:  \"Centrosome\",   \n",
    "    20:  \"Lipid droplets\",   \n",
    "    21:  \"Plasma membrane\",   \n",
    "    22:  \"Cell junctions\",   \n",
    "    23:  \"Mitochondria\",   \n",
    "    24:  \"Aggresome\",   \n",
    "    25:  \"Cytosol\",   \n",
    "    26:  \"Cytoplasmic bodies\",   \n",
    "    27:  \"Rods & rings\"\n",
    "}\n",
    "# reverse_names_dict = dict((v,k) for k,v in names_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_targets(row):\n",
    "    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n",
    "    for num in row.Target:\n",
    "        name = names_dict[int(num)]\n",
    "        row.loc[name] = 1\n",
    "    return row\n",
    "\n",
    "for key in names_dict.keys():\n",
    "    train_labels[names_dict[key]] = 0\n",
    "train_labels = train_labels.apply(fill_targets, axis=1)\n",
    "train_labels[\"number_of_targets\"] = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "for train_idx, test_idx in kf.split(train_labels.index.values):\n",
    "    partition = {}\n",
    "    partition[\"train\"] = train_labels.Id.values[train_idx]\n",
    "    partition[\"validation\"] = train_labels.Id.values[test_idx]\n",
    "#     X_train, X_test = train_labels.Id.values[train_idx], train_labels.Id.values[test_idx]\n",
    "#     y_train, y_test = train_labels.Id.values[train_idx], train_labels.Id.values[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array(['00070df0-bbc3-11e8-b2bc-ac1f6b6435d0',\n",
       "        '000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0',\n",
       "        '000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', ...,\n",
       "        'ab33dff8-bba7-11e8-b2ba-ac1f6b6435d0',\n",
       "        'ab351f1c-bbb6-11e8-b2ba-ac1f6b6435d0',\n",
       "        'ab385b2e-bbba-11e8-b2ba-ac1f6b6435d0'], dtype=object),\n",
       " 'validation': array(['ab3978aa-bbbb-11e8-b2ba-ac1f6b6435d0',\n",
       "        'ab3b9258-bbab-11e8-b2ba-ac1f6b6435d0',\n",
       "        'ab3ca16e-bbc1-11e8-b2bb-ac1f6b6435d0', ...,\n",
       "        'fff189d8-bbab-11e8-b2ba-ac1f6b6435d0',\n",
       "        'fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0',\n",
       "        'fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0'], dtype=object)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameters(object):\n",
    "    path = train_path\n",
    "    num_classes=28\n",
    "    image_rows=512\n",
    "    image_cols=512\n",
    "    batch_size=100\n",
    "    n_channels=3\n",
    "    shuffle=False\n",
    "    scaled_row_dim = 139\n",
    "    scaled_col_dim = 139 \n",
    "    n_epochs=10\n",
    "\n",
    "parameter = ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePreprocessor:\n",
    "    \n",
    "    def __init__(self, modelparameter):\n",
    "        self.parameter = modelparameter\n",
    "        self.path = self.parameter.path\n",
    "        self.scaled_row_dim = self.parameter.scaled_row_dim\n",
    "        self.scaled_col_dim = self.parameter.scaled_col_dim\n",
    "        self.n_channels = self.parameter.n_channels\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        image = self.resize(image)\n",
    "        image = self.reshape(image)\n",
    "        image = self.normalize(image)\n",
    "        return image\n",
    "    \n",
    "    def resize(self, image):\n",
    "        return resize(image, (self.scaled_row_dim, self.scaled_col_dim))\n",
    "    \n",
    "    def reshape(self, image):\n",
    "        return np.reshape(image, (image.shape[0], image.shape[1], self.n_channels))\n",
    "    \n",
    "    def normalize(self, image):\n",
    "#         image /= 255\n",
    "#         return image\n",
    "        return (image / 255.0 - 0.5) / 0.5\n",
    "            \n",
    "    def load_image(self, image_id):\n",
    "        image = np.zeros(shape=(512,512,4))\n",
    "        image[:,:,0] = imread(self.basepath + image_id + \"_green\" + \".png\")\n",
    "        image[:,:,1] = imread(self.basepath + image_id + \"_blue\" + \".png\")\n",
    "        image[:,:,2] = imread(self.basepath + image_id + \"_red\" + \".png\")\n",
    "        image[:,:,3] = imread(self.basepath + image_id + \"_yellow\" + \".png\")\n",
    "        return image[:,:,0:self.parameter.n_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ImagePreprocessor(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor):\n",
    "        self.params = modelparameter\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dim = (self.params.scaled_row_dim, self.params.scaled_col_dim)\n",
    "        self.batch_size = self.params.batch_size\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.num_classes = self.params.num_classes\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.shuffle = self.params.shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier].drop(\n",
    "                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n",
    "        for i, identifier in enumerate(list_IDs_temp):\n",
    "            image = self.preprocessor.load_image(identifier)\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            X[i] = image\n",
    "            y[i] = self.get_targets_per_image(identifier)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictGenerator:\n",
    "    \n",
    "    def __init__(self, predict_Ids, imagepreprocessor, predict_path):\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.preprocessor.basepath = predict_path\n",
    "        self.identifiers = predict_Ids\n",
    "    \n",
    "    def predict(self, model):\n",
    "        y = np.empty(shape=(len(self.identifiers), self.preprocessor.parameter.num_classes))\n",
    "        for n in range(len(self.identifiers)):\n",
    "            image = self.preprocessor.load_image(self.identifiers[n])\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            image = image.reshape((1, *image.shape))\n",
    "            y[n] = model.predict(image)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.models import load_model\n",
    "\n",
    "class BaseLineModel:\n",
    "    \n",
    "    def __init__(self, modelparameter):\n",
    "        self.params = modelparameter\n",
    "        self.num_classes = self.params.num_classes\n",
    "        self.img_rows = self.params.scaled_row_dim\n",
    "        self.img_cols = self.params.scaled_col_dim\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.input_shape = (self.img_rows, self.img_cols, self.n_channels)\n",
    "        self.my_metrics = ['accuracy']\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n",
    "        self.model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(self.num_classes, activation='sigmoid'))\n",
    "\n",
    "    def compile_model(self):\n",
    "        self.model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=self.my_metrics)\n",
    "    \n",
    "    def set_generators(self, train_generator, validation_generator):\n",
    "        self.training_generator = train_generator\n",
    "        self.validation_generator = validation_generator\n",
    "    \n",
    "    def learn(self):\n",
    "        return self.model.fit_generator(generator=self.training_generator,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    epochs=self.params.n_epochs, \n",
    "                    steps_per_epoch=100,\n",
    "                    use_multiprocessing=True,\n",
    "                    validation_steps=50,\n",
    "                    workers=8)\n",
    "    \n",
    "    def score(self):\n",
    "        return self.model.evaluate_generator(generator=self.validation_generator,\n",
    "                                      use_multiprocessing=True, \n",
    "                                      workers=8)\n",
    "\n",
    "    def predict(self, predict_generator):\n",
    "        y = predict_generator.predict(self.model)\n",
    "        return y\n",
    "    \n",
    "    def save(self, modeloutputpath):\n",
    "        self.model.save(modeloutputpath)\n",
    "    \n",
    "    def load(self, modelinputpath):\n",
    "        self.model = load_model(modelinputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train: 20715\n",
      "Number of samples in validation: 10357\n"
     ]
    }
   ],
   "source": [
    "labels = train_labels\n",
    "print(\"Number of samples in train: {}\".format(len(partition[\"train\"])))\n",
    "print(\"Number of samples in validation: {}\".format(len(partition[\"validation\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels, parameter, preprocessor)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, parameter, preprocessor)\n",
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8fe276b23103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#model.save(\"baseline_model.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mproba_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-a0b8ee2eff3c>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     workers=8)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2269\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m                 config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n\u001b[1;32m    162\u001b[0m                                         allow_soft_placement=True)\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0m_SESSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \"\"\"\n\u001b[0;32m-> 1494\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model = BaseLineModel(parameter)\n",
    "model.build_model()\n",
    "model.compile_model()\n",
    "model.set_generators(training_generator, validation_generator)\n",
    "history = model.learn()\n",
    "#model.save(\"baseline_model.h5\")\n",
    "proba_predictions = model.predict(predict_generator)\n",
    "baseline_proba_predictions = pd.DataFrame(proba_predictions, columns=train_labels.drop(\n",
    "    [\"Target\", \"number_of_targets\", \"Id\"], axis=1).columns)\n",
    "baseline_proba_predictions.to_csv(\"baseline_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_proba_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14510786567249704164\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11273211085\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 6308853822187419784\n",
      "physical_device_desc: \"device: 0, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11273211085\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 4448833557162495234\n",
      "physical_device_desc: \"device: 1, name: Tesla K40m, pci bus id: 0000:04:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11273211085\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 3\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 10515289147129957701\n",
      "physical_device_desc: \"device: 2, name: Tesla K40m, pci bus id: 0000:82:00.0, compute capability: 3.5\"\n",
      ", name: \"/device:GPU:3\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11273211085\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 4285106612030023056\n",
      "physical_device_desc: \"device: 3, name: Tesla K40m, pci bus id: 0000:83:00.0, compute capability: 3.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def base_f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return f1\n",
    "\n",
    "def f1_min(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.min(f1)\n",
    "\n",
    "def f1_max(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.max(f1)\n",
    "\n",
    "def f1_mean(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_std(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.std(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackHistory(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDataGenerator(DataGenerator):\n",
    "    \n",
    "    # in contrast to the base DataGenerator we add a target wishlist to init\n",
    "    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor, target_wishlist):\n",
    "        super().__init__(list_IDs, labels, modelparameter, imagepreprocessor)\n",
    "        self.target_wishlist = target_wishlist\n",
    "    \n",
    "    def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier][self.target_wishlist].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dropout = False\n",
    "from keras.models import Model, Sequential,load_model\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.losses import binary_crossentropy\n",
    "# from inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.optimizers import Adadelta,Adam\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "\n",
    "class ImprovedModel(BaseLineModel):\n",
    "    \n",
    "    def __init__(self, modelparameter,\n",
    "                 use_dropout,\n",
    "                 my_metrics=[f1_mean, f1_std, f1_min, f1_max,'accuracy']):\n",
    "        \n",
    "        super().__init__(modelparameter)\n",
    "        self.my_metrics = my_metrics\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def learn(self):\n",
    "        self.history = TrackHistory()\n",
    "        return self.model.fit_generator(generator=self.training_generator,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    epochs=self.params.n_epochs, \n",
    "                    use_multiprocessing=True,\n",
    "                    workers=8,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_steps=50,                                     \n",
    "                    callbacks = [self.history])\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n",
    "        self.model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(self.num_classes, activation='sigmoid'))       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = ModelParameters()\n",
    "preprocessor = ImagePreprocessor(parameter)\n",
    "labels = train_labels\n",
    "training_generator = DataGenerator(partition['train'], labels,\n",
    "                                           parameter, preprocessor)\n",
    "validation_generator = DataGenerator(partition['validation'], labels,\n",
    "                                             parameter, preprocessor)\n",
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(intra_op_parallelism_threads=2,\\\n",
    "        inter_op_parallelism_threads=2, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : 2, 'GPU' : 1})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1188: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1171: calling reduce_min (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1154: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 121s - loss: 0.6004 - f1_mean: 0.0653 - f1_std: 0.1096 - f1_min: 0.0000e+00 - f1_max: 0.4669 - acc: 0.7039 - val_loss: 0.4193 - val_f1_mean: 0.0021 - val_f1_std: 0.0098 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0506 - val_acc: 0.9067\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 110s - loss: 0.4636 - f1_mean: 0.0598 - f1_std: 0.0991 - f1_min: 0.0000e+00 - f1_max: 0.3984 - acc: 0.7916 - val_loss: 0.3217 - val_f1_mean: 4.8860e-04 - val_f1_std: 0.0025 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0137 - val_acc: 0.9412\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 111s - loss: 0.3789 - f1_mean: 0.0587 - f1_std: 0.1004 - f1_min: 0.0000e+00 - f1_max: 0.3855 - acc: 0.8377 - val_loss: 0.2109 - val_f1_mean: 4.5947e-04 - val_f1_std: 0.0024 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0129 - val_acc: 0.9412\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 112s - loss: 0.3112 - f1_mean: 0.0489 - f1_std: 0.0979 - f1_min: 0.0000e+00 - f1_max: 0.3741 - acc: 0.8838 - val_loss: 0.1835 - val_f1_mean: 3.0215e-04 - val_f1_std: 0.0016 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0085 - val_acc: 0.9412\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 110s - loss: 0.2725 - f1_mean: 0.0381 - f1_std: 0.0837 - f1_min: 0.0000e+00 - f1_max: 0.3164 - acc: 0.9085 - val_loss: 0.1814 - val_f1_mean: 4.4643e-05 - val_f1_std: 2.3197e-04 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0012 - val_acc: 0.9413\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 110s - loss: 0.2442 - f1_mean: 0.0244 - f1_std: 0.0612 - f1_min: 0.0000e+00 - f1_max: 0.2418 - acc: 0.9223 - val_loss: 0.1762 - val_f1_mean: 0.0000e+00 - val_f1_std: 0.0000e+00 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0000e+00 - val_acc: 0.9413\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 110s - loss: 0.2289 - f1_mean: 0.0112 - f1_std: 0.0377 - f1_min: 0.0000e+00 - f1_max: 0.1754 - acc: 0.9325 - val_loss: 0.1762 - val_f1_mean: 0.0000e+00 - val_f1_std: 0.0000e+00 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0000e+00 - val_acc: 0.9413\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 113s - loss: 0.2169 - f1_mean: 0.0033 - f1_std: 0.0147 - f1_min: 0.0000e+00 - f1_max: 0.0752 - acc: 0.9395 - val_loss: 0.1756 - val_f1_mean: 0.0000e+00 - val_f1_std: 0.0000e+00 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0000e+00 - val_acc: 0.9413\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 112s - loss: 0.2106 - f1_mean: 3.9683e-04 - f1_std: 0.0021 - f1_min: 0.0000e+00 - f1_max: 0.0111 - acc: 0.9413 - val_loss: 0.1757 - val_f1_mean: 0.0000e+00 - val_f1_std: 0.0000e+00 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0000e+00 - val_acc: 0.9413\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 111s - loss: 0.2064 - f1_mean: 0.0000e+00 - f1_std: 0.0000e+00 - f1_min: 0.0000e+00 - f1_max: 0.0000e+00 - acc: 0.9420 - val_loss: 0.1748 - val_f1_mean: 0.0000e+00 - val_f1_std: 0.0000e+00 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0000e+00 - val_acc: 0.9413\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model = ImprovedModel(parameter, use_dropout=use_dropout)\n",
    "model.build_model()\n",
    "model.compile_model()\n",
    "model.set_generators(training_generator, validation_generator)\n",
    "epoch_history = model.learn()\n",
    "proba_predictions = model.predict(predict_generator)\n",
    "#model.save(\"improved_model.h5\")\n",
    "improved_proba_predictions = pd.DataFrame(proba_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.435248</td>\n",
       "      <td>0.056095</td>\n",
       "      <td>0.160077</td>\n",
       "      <td>0.068470</td>\n",
       "      <td>0.075845</td>\n",
       "      <td>0.121998</td>\n",
       "      <td>0.046047</td>\n",
       "      <td>0.131524</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.007955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047340</td>\n",
       "      <td>0.076626</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.161534</td>\n",
       "      <td>0.037209</td>\n",
       "      <td>0.139064</td>\n",
       "      <td>0.014042</td>\n",
       "      <td>0.308225</td>\n",
       "      <td>0.015129</td>\n",
       "      <td>0.003911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.431858</td>\n",
       "      <td>0.050662</td>\n",
       "      <td>0.128042</td>\n",
       "      <td>0.046942</td>\n",
       "      <td>0.040974</td>\n",
       "      <td>0.093354</td>\n",
       "      <td>0.044119</td>\n",
       "      <td>0.106936</td>\n",
       "      <td>0.002390</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027585</td>\n",
       "      <td>0.060010</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.160404</td>\n",
       "      <td>0.027471</td>\n",
       "      <td>0.105933</td>\n",
       "      <td>0.008125</td>\n",
       "      <td>0.316956</td>\n",
       "      <td>0.008367</td>\n",
       "      <td>0.002491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.429936</td>\n",
       "      <td>0.039634</td>\n",
       "      <td>0.138280</td>\n",
       "      <td>0.052903</td>\n",
       "      <td>0.060935</td>\n",
       "      <td>0.100455</td>\n",
       "      <td>0.030663</td>\n",
       "      <td>0.108412</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033643</td>\n",
       "      <td>0.056420</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.136009</td>\n",
       "      <td>0.024523</td>\n",
       "      <td>0.117712</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>0.286155</td>\n",
       "      <td>0.008878</td>\n",
       "      <td>0.001747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.429860</td>\n",
       "      <td>0.039201</td>\n",
       "      <td>0.138427</td>\n",
       "      <td>0.052335</td>\n",
       "      <td>0.059751</td>\n",
       "      <td>0.099938</td>\n",
       "      <td>0.030040</td>\n",
       "      <td>0.106245</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033047</td>\n",
       "      <td>0.055610</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.135393</td>\n",
       "      <td>0.023953</td>\n",
       "      <td>0.116278</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.285410</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>0.001719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.434904</td>\n",
       "      <td>0.061005</td>\n",
       "      <td>0.144354</td>\n",
       "      <td>0.058547</td>\n",
       "      <td>0.052167</td>\n",
       "      <td>0.108814</td>\n",
       "      <td>0.053142</td>\n",
       "      <td>0.121740</td>\n",
       "      <td>0.003687</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036010</td>\n",
       "      <td>0.071629</td>\n",
       "      <td>0.005306</td>\n",
       "      <td>0.174064</td>\n",
       "      <td>0.035118</td>\n",
       "      <td>0.121988</td>\n",
       "      <td>0.011695</td>\n",
       "      <td>0.324707</td>\n",
       "      <td>0.012027</td>\n",
       "      <td>0.003812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.432274</td>\n",
       "      <td>0.050722</td>\n",
       "      <td>0.138208</td>\n",
       "      <td>0.053917</td>\n",
       "      <td>0.051736</td>\n",
       "      <td>0.102044</td>\n",
       "      <td>0.042046</td>\n",
       "      <td>0.113366</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032809</td>\n",
       "      <td>0.062744</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>0.157552</td>\n",
       "      <td>0.029179</td>\n",
       "      <td>0.116717</td>\n",
       "      <td>0.009260</td>\n",
       "      <td>0.309145</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.002563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.430631</td>\n",
       "      <td>0.044036</td>\n",
       "      <td>0.137183</td>\n",
       "      <td>0.052785</td>\n",
       "      <td>0.055083</td>\n",
       "      <td>0.100297</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>0.109370</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.004440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032656</td>\n",
       "      <td>0.058134</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.144678</td>\n",
       "      <td>0.026004</td>\n",
       "      <td>0.116429</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.295423</td>\n",
       "      <td>0.009056</td>\n",
       "      <td>0.002003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.431717</td>\n",
       "      <td>0.041810</td>\n",
       "      <td>0.146072</td>\n",
       "      <td>0.055348</td>\n",
       "      <td>0.064299</td>\n",
       "      <td>0.105269</td>\n",
       "      <td>0.032730</td>\n",
       "      <td>0.109090</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035917</td>\n",
       "      <td>0.060343</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>0.140703</td>\n",
       "      <td>0.026173</td>\n",
       "      <td>0.119453</td>\n",
       "      <td>0.008742</td>\n",
       "      <td>0.290713</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.002166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.433062</td>\n",
       "      <td>0.046025</td>\n",
       "      <td>0.145592</td>\n",
       "      <td>0.055576</td>\n",
       "      <td>0.061132</td>\n",
       "      <td>0.106047</td>\n",
       "      <td>0.038009</td>\n",
       "      <td>0.112523</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036042</td>\n",
       "      <td>0.064348</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.149204</td>\n",
       "      <td>0.028629</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.301247</td>\n",
       "      <td>0.010377</td>\n",
       "      <td>0.002662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.436490</td>\n",
       "      <td>0.060621</td>\n",
       "      <td>0.156116</td>\n",
       "      <td>0.064372</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.117707</td>\n",
       "      <td>0.052771</td>\n",
       "      <td>0.126958</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042293</td>\n",
       "      <td>0.077666</td>\n",
       "      <td>0.006347</td>\n",
       "      <td>0.172340</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.130054</td>\n",
       "      <td>0.013914</td>\n",
       "      <td>0.321646</td>\n",
       "      <td>0.014152</td>\n",
       "      <td>0.004515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.432936</td>\n",
       "      <td>0.049771</td>\n",
       "      <td>0.146273</td>\n",
       "      <td>0.058225</td>\n",
       "      <td>0.060401</td>\n",
       "      <td>0.108433</td>\n",
       "      <td>0.040246</td>\n",
       "      <td>0.116596</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037271</td>\n",
       "      <td>0.065515</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.153972</td>\n",
       "      <td>0.030280</td>\n",
       "      <td>0.123475</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>0.303652</td>\n",
       "      <td>0.011061</td>\n",
       "      <td>0.002779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.431748</td>\n",
       "      <td>0.041220</td>\n",
       "      <td>0.148251</td>\n",
       "      <td>0.057299</td>\n",
       "      <td>0.070271</td>\n",
       "      <td>0.107867</td>\n",
       "      <td>0.032217</td>\n",
       "      <td>0.113083</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038623</td>\n",
       "      <td>0.061980</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.138341</td>\n",
       "      <td>0.027015</td>\n",
       "      <td>0.124020</td>\n",
       "      <td>0.009510</td>\n",
       "      <td>0.287833</td>\n",
       "      <td>0.010424</td>\n",
       "      <td>0.002205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.431645</td>\n",
       "      <td>0.043346</td>\n",
       "      <td>0.150466</td>\n",
       "      <td>0.060802</td>\n",
       "      <td>0.071509</td>\n",
       "      <td>0.111092</td>\n",
       "      <td>0.032784</td>\n",
       "      <td>0.115586</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040110</td>\n",
       "      <td>0.062210</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.140597</td>\n",
       "      <td>0.027848</td>\n",
       "      <td>0.128521</td>\n",
       "      <td>0.009682</td>\n",
       "      <td>0.287275</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>0.002228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.433727</td>\n",
       "      <td>0.053516</td>\n",
       "      <td>0.147114</td>\n",
       "      <td>0.059293</td>\n",
       "      <td>0.059856</td>\n",
       "      <td>0.109784</td>\n",
       "      <td>0.044460</td>\n",
       "      <td>0.120171</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037885</td>\n",
       "      <td>0.068534</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.160713</td>\n",
       "      <td>0.032453</td>\n",
       "      <td>0.124744</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.310493</td>\n",
       "      <td>0.011727</td>\n",
       "      <td>0.003131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.431604</td>\n",
       "      <td>0.043182</td>\n",
       "      <td>0.153448</td>\n",
       "      <td>0.063144</td>\n",
       "      <td>0.077551</td>\n",
       "      <td>0.113932</td>\n",
       "      <td>0.032267</td>\n",
       "      <td>0.119077</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042807</td>\n",
       "      <td>0.063499</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>0.139214</td>\n",
       "      <td>0.028636</td>\n",
       "      <td>0.133140</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>0.284677</td>\n",
       "      <td>0.011699</td>\n",
       "      <td>0.002238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.435097</td>\n",
       "      <td>0.063999</td>\n",
       "      <td>0.141487</td>\n",
       "      <td>0.054711</td>\n",
       "      <td>0.047515</td>\n",
       "      <td>0.106032</td>\n",
       "      <td>0.057685</td>\n",
       "      <td>0.123307</td>\n",
       "      <td>0.004159</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035508</td>\n",
       "      <td>0.074899</td>\n",
       "      <td>0.005640</td>\n",
       "      <td>0.179197</td>\n",
       "      <td>0.036986</td>\n",
       "      <td>0.119217</td>\n",
       "      <td>0.011088</td>\n",
       "      <td>0.332750</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>0.004347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.435273</td>\n",
       "      <td>0.063456</td>\n",
       "      <td>0.154552</td>\n",
       "      <td>0.067550</td>\n",
       "      <td>0.066239</td>\n",
       "      <td>0.119246</td>\n",
       "      <td>0.053413</td>\n",
       "      <td>0.134421</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044185</td>\n",
       "      <td>0.077394</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.173774</td>\n",
       "      <td>0.039452</td>\n",
       "      <td>0.137062</td>\n",
       "      <td>0.014059</td>\n",
       "      <td>0.319705</td>\n",
       "      <td>0.014997</td>\n",
       "      <td>0.004118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.430049</td>\n",
       "      <td>0.040746</td>\n",
       "      <td>0.134843</td>\n",
       "      <td>0.050787</td>\n",
       "      <td>0.054432</td>\n",
       "      <td>0.097663</td>\n",
       "      <td>0.032009</td>\n",
       "      <td>0.105351</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031175</td>\n",
       "      <td>0.055463</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.139195</td>\n",
       "      <td>0.024106</td>\n",
       "      <td>0.113191</td>\n",
       "      <td>0.007731</td>\n",
       "      <td>0.290749</td>\n",
       "      <td>0.008350</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.435239</td>\n",
       "      <td>0.059838</td>\n",
       "      <td>0.154025</td>\n",
       "      <td>0.064883</td>\n",
       "      <td>0.065887</td>\n",
       "      <td>0.117132</td>\n",
       "      <td>0.050705</td>\n",
       "      <td>0.130138</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>0.008271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043022</td>\n",
       "      <td>0.076059</td>\n",
       "      <td>0.006049</td>\n",
       "      <td>0.169405</td>\n",
       "      <td>0.037606</td>\n",
       "      <td>0.133286</td>\n",
       "      <td>0.013376</td>\n",
       "      <td>0.317339</td>\n",
       "      <td>0.014189</td>\n",
       "      <td>0.003987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.430839</td>\n",
       "      <td>0.040667</td>\n",
       "      <td>0.142612</td>\n",
       "      <td>0.054194</td>\n",
       "      <td>0.061094</td>\n",
       "      <td>0.102835</td>\n",
       "      <td>0.031195</td>\n",
       "      <td>0.106338</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034075</td>\n",
       "      <td>0.057311</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>0.138352</td>\n",
       "      <td>0.024741</td>\n",
       "      <td>0.117410</td>\n",
       "      <td>0.008076</td>\n",
       "      <td>0.287812</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.001922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.433756</td>\n",
       "      <td>0.051259</td>\n",
       "      <td>0.159084</td>\n",
       "      <td>0.066979</td>\n",
       "      <td>0.078231</td>\n",
       "      <td>0.119709</td>\n",
       "      <td>0.040241</td>\n",
       "      <td>0.127892</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046456</td>\n",
       "      <td>0.071936</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>0.153311</td>\n",
       "      <td>0.034144</td>\n",
       "      <td>0.138198</td>\n",
       "      <td>0.012166</td>\n",
       "      <td>0.298970</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>0.003150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.432867</td>\n",
       "      <td>0.047160</td>\n",
       "      <td>0.154916</td>\n",
       "      <td>0.062519</td>\n",
       "      <td>0.073569</td>\n",
       "      <td>0.114156</td>\n",
       "      <td>0.036670</td>\n",
       "      <td>0.119885</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042085</td>\n",
       "      <td>0.066816</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.148142</td>\n",
       "      <td>0.030589</td>\n",
       "      <td>0.130797</td>\n",
       "      <td>0.010468</td>\n",
       "      <td>0.294945</td>\n",
       "      <td>0.011978</td>\n",
       "      <td>0.002652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.438496</td>\n",
       "      <td>0.083501</td>\n",
       "      <td>0.149868</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>0.049040</td>\n",
       "      <td>0.116312</td>\n",
       "      <td>0.079708</td>\n",
       "      <td>0.136735</td>\n",
       "      <td>0.006578</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039516</td>\n",
       "      <td>0.087385</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>0.207062</td>\n",
       "      <td>0.047242</td>\n",
       "      <td>0.127955</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.354746</td>\n",
       "      <td>0.015736</td>\n",
       "      <td>0.006838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.432655</td>\n",
       "      <td>0.046435</td>\n",
       "      <td>0.155723</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.077802</td>\n",
       "      <td>0.116051</td>\n",
       "      <td>0.035712</td>\n",
       "      <td>0.122417</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043904</td>\n",
       "      <td>0.066955</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.145564</td>\n",
       "      <td>0.030811</td>\n",
       "      <td>0.134530</td>\n",
       "      <td>0.011087</td>\n",
       "      <td>0.291457</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.002588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.432656</td>\n",
       "      <td>0.051009</td>\n",
       "      <td>0.142802</td>\n",
       "      <td>0.056659</td>\n",
       "      <td>0.056598</td>\n",
       "      <td>0.105937</td>\n",
       "      <td>0.041745</td>\n",
       "      <td>0.116440</td>\n",
       "      <td>0.002621</td>\n",
       "      <td>0.005782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035548</td>\n",
       "      <td>0.064836</td>\n",
       "      <td>0.004199</td>\n",
       "      <td>0.156770</td>\n",
       "      <td>0.030293</td>\n",
       "      <td>0.121299</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.307031</td>\n",
       "      <td>0.010680</td>\n",
       "      <td>0.002716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.433489</td>\n",
       "      <td>0.057770</td>\n",
       "      <td>0.144930</td>\n",
       "      <td>0.060473</td>\n",
       "      <td>0.059606</td>\n",
       "      <td>0.110872</td>\n",
       "      <td>0.048632</td>\n",
       "      <td>0.129041</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040038</td>\n",
       "      <td>0.072050</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>0.165369</td>\n",
       "      <td>0.035697</td>\n",
       "      <td>0.130050</td>\n",
       "      <td>0.011941</td>\n",
       "      <td>0.314930</td>\n",
       "      <td>0.012996</td>\n",
       "      <td>0.003392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.429613</td>\n",
       "      <td>0.041336</td>\n",
       "      <td>0.133041</td>\n",
       "      <td>0.050046</td>\n",
       "      <td>0.052695</td>\n",
       "      <td>0.096649</td>\n",
       "      <td>0.032347</td>\n",
       "      <td>0.106215</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030937</td>\n",
       "      <td>0.055352</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>0.139846</td>\n",
       "      <td>0.024289</td>\n",
       "      <td>0.113205</td>\n",
       "      <td>0.007449</td>\n",
       "      <td>0.291516</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.001758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.430025</td>\n",
       "      <td>0.039950</td>\n",
       "      <td>0.141805</td>\n",
       "      <td>0.055860</td>\n",
       "      <td>0.065414</td>\n",
       "      <td>0.103872</td>\n",
       "      <td>0.030192</td>\n",
       "      <td>0.110738</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035782</td>\n",
       "      <td>0.057170</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.135305</td>\n",
       "      <td>0.025079</td>\n",
       "      <td>0.122160</td>\n",
       "      <td>0.008498</td>\n",
       "      <td>0.283419</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>0.001770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.435004</td>\n",
       "      <td>0.054341</td>\n",
       "      <td>0.159012</td>\n",
       "      <td>0.066150</td>\n",
       "      <td>0.075515</td>\n",
       "      <td>0.119828</td>\n",
       "      <td>0.044927</td>\n",
       "      <td>0.129886</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046380</td>\n",
       "      <td>0.075928</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.159672</td>\n",
       "      <td>0.036360</td>\n",
       "      <td>0.136609</td>\n",
       "      <td>0.013382</td>\n",
       "      <td>0.307551</td>\n",
       "      <td>0.014536</td>\n",
       "      <td>0.003754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.437162</td>\n",
       "      <td>0.057857</td>\n",
       "      <td>0.172369</td>\n",
       "      <td>0.074732</td>\n",
       "      <td>0.088967</td>\n",
       "      <td>0.130632</td>\n",
       "      <td>0.048004</td>\n",
       "      <td>0.135454</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053010</td>\n",
       "      <td>0.082648</td>\n",
       "      <td>0.007253</td>\n",
       "      <td>0.164945</td>\n",
       "      <td>0.040153</td>\n",
       "      <td>0.145185</td>\n",
       "      <td>0.016439</td>\n",
       "      <td>0.309875</td>\n",
       "      <td>0.017210</td>\n",
       "      <td>0.004671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10327</th>\n",
       "      <td>0.433208</td>\n",
       "      <td>0.050479</td>\n",
       "      <td>0.151202</td>\n",
       "      <td>0.062301</td>\n",
       "      <td>0.067355</td>\n",
       "      <td>0.113265</td>\n",
       "      <td>0.040167</td>\n",
       "      <td>0.121417</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.006057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040854</td>\n",
       "      <td>0.067753</td>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.153564</td>\n",
       "      <td>0.031771</td>\n",
       "      <td>0.129992</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.301174</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>0.002892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10328</th>\n",
       "      <td>0.433867</td>\n",
       "      <td>0.049614</td>\n",
       "      <td>0.149663</td>\n",
       "      <td>0.059294</td>\n",
       "      <td>0.062615</td>\n",
       "      <td>0.110163</td>\n",
       "      <td>0.040658</td>\n",
       "      <td>0.115204</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037574</td>\n",
       "      <td>0.066457</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>0.154907</td>\n",
       "      <td>0.030347</td>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.010704</td>\n",
       "      <td>0.304718</td>\n",
       "      <td>0.011187</td>\n",
       "      <td>0.002950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10329</th>\n",
       "      <td>0.430428</td>\n",
       "      <td>0.041213</td>\n",
       "      <td>0.138525</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>0.058685</td>\n",
       "      <td>0.100294</td>\n",
       "      <td>0.032383</td>\n",
       "      <td>0.108522</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033397</td>\n",
       "      <td>0.057781</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.139558</td>\n",
       "      <td>0.025243</td>\n",
       "      <td>0.116535</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>0.290671</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.001904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10330</th>\n",
       "      <td>0.432583</td>\n",
       "      <td>0.049688</td>\n",
       "      <td>0.148011</td>\n",
       "      <td>0.058424</td>\n",
       "      <td>0.062270</td>\n",
       "      <td>0.109245</td>\n",
       "      <td>0.039445</td>\n",
       "      <td>0.117968</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.005507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038551</td>\n",
       "      <td>0.066109</td>\n",
       "      <td>0.004304</td>\n",
       "      <td>0.153109</td>\n",
       "      <td>0.030634</td>\n",
       "      <td>0.125347</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>0.301998</td>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.002746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10331</th>\n",
       "      <td>0.430546</td>\n",
       "      <td>0.038780</td>\n",
       "      <td>0.141268</td>\n",
       "      <td>0.052991</td>\n",
       "      <td>0.064223</td>\n",
       "      <td>0.101760</td>\n",
       "      <td>0.030373</td>\n",
       "      <td>0.108401</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034825</td>\n",
       "      <td>0.057923</td>\n",
       "      <td>0.003216</td>\n",
       "      <td>0.134805</td>\n",
       "      <td>0.024773</td>\n",
       "      <td>0.117986</td>\n",
       "      <td>0.008402</td>\n",
       "      <td>0.285735</td>\n",
       "      <td>0.009116</td>\n",
       "      <td>0.001858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10332</th>\n",
       "      <td>0.431392</td>\n",
       "      <td>0.041822</td>\n",
       "      <td>0.145292</td>\n",
       "      <td>0.055143</td>\n",
       "      <td>0.064979</td>\n",
       "      <td>0.105189</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.111249</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.004250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036722</td>\n",
       "      <td>0.060968</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>0.139962</td>\n",
       "      <td>0.026622</td>\n",
       "      <td>0.120955</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>0.290151</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>0.002160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10333</th>\n",
       "      <td>0.429627</td>\n",
       "      <td>0.038024</td>\n",
       "      <td>0.138047</td>\n",
       "      <td>0.052492</td>\n",
       "      <td>0.061163</td>\n",
       "      <td>0.099857</td>\n",
       "      <td>0.029020</td>\n",
       "      <td>0.106060</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033169</td>\n",
       "      <td>0.054899</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.132867</td>\n",
       "      <td>0.023494</td>\n",
       "      <td>0.116693</td>\n",
       "      <td>0.007822</td>\n",
       "      <td>0.282671</td>\n",
       "      <td>0.008560</td>\n",
       "      <td>0.001641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10334</th>\n",
       "      <td>0.436704</td>\n",
       "      <td>0.048575</td>\n",
       "      <td>0.166225</td>\n",
       "      <td>0.066709</td>\n",
       "      <td>0.079951</td>\n",
       "      <td>0.121322</td>\n",
       "      <td>0.040433</td>\n",
       "      <td>0.116914</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043949</td>\n",
       "      <td>0.072559</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.153827</td>\n",
       "      <td>0.032197</td>\n",
       "      <td>0.129296</td>\n",
       "      <td>0.013704</td>\n",
       "      <td>0.302452</td>\n",
       "      <td>0.013139</td>\n",
       "      <td>0.003724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335</th>\n",
       "      <td>0.433308</td>\n",
       "      <td>0.055586</td>\n",
       "      <td>0.150810</td>\n",
       "      <td>0.064437</td>\n",
       "      <td>0.066108</td>\n",
       "      <td>0.115178</td>\n",
       "      <td>0.044407</td>\n",
       "      <td>0.128017</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.070643</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.160479</td>\n",
       "      <td>0.034656</td>\n",
       "      <td>0.134420</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.306791</td>\n",
       "      <td>0.013289</td>\n",
       "      <td>0.003167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10336</th>\n",
       "      <td>0.436506</td>\n",
       "      <td>0.048446</td>\n",
       "      <td>0.168307</td>\n",
       "      <td>0.068535</td>\n",
       "      <td>0.085477</td>\n",
       "      <td>0.123407</td>\n",
       "      <td>0.039953</td>\n",
       "      <td>0.120394</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046312</td>\n",
       "      <td>0.073741</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>0.152648</td>\n",
       "      <td>0.033018</td>\n",
       "      <td>0.133249</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.300307</td>\n",
       "      <td>0.013801</td>\n",
       "      <td>0.003678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10337</th>\n",
       "      <td>0.430632</td>\n",
       "      <td>0.041806</td>\n",
       "      <td>0.140089</td>\n",
       "      <td>0.054069</td>\n",
       "      <td>0.060427</td>\n",
       "      <td>0.102143</td>\n",
       "      <td>0.032614</td>\n",
       "      <td>0.109707</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034203</td>\n",
       "      <td>0.058141</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.140158</td>\n",
       "      <td>0.025610</td>\n",
       "      <td>0.118643</td>\n",
       "      <td>0.008478</td>\n",
       "      <td>0.290190</td>\n",
       "      <td>0.009263</td>\n",
       "      <td>0.001934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10338</th>\n",
       "      <td>0.430396</td>\n",
       "      <td>0.041406</td>\n",
       "      <td>0.139064</td>\n",
       "      <td>0.053413</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.101214</td>\n",
       "      <td>0.032177</td>\n",
       "      <td>0.108665</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033595</td>\n",
       "      <td>0.057344</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.139492</td>\n",
       "      <td>0.025174</td>\n",
       "      <td>0.117649</td>\n",
       "      <td>0.008220</td>\n",
       "      <td>0.289599</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>0.001876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10339</th>\n",
       "      <td>0.430518</td>\n",
       "      <td>0.040860</td>\n",
       "      <td>0.145616</td>\n",
       "      <td>0.058281</td>\n",
       "      <td>0.070886</td>\n",
       "      <td>0.107411</td>\n",
       "      <td>0.030960</td>\n",
       "      <td>0.115245</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.004369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038782</td>\n",
       "      <td>0.059973</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.136082</td>\n",
       "      <td>0.026621</td>\n",
       "      <td>0.126944</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>0.283622</td>\n",
       "      <td>0.010381</td>\n",
       "      <td>0.001930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>0.432337</td>\n",
       "      <td>0.042609</td>\n",
       "      <td>0.143740</td>\n",
       "      <td>0.054449</td>\n",
       "      <td>0.062591</td>\n",
       "      <td>0.103760</td>\n",
       "      <td>0.034939</td>\n",
       "      <td>0.109567</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.061134</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.143856</td>\n",
       "      <td>0.026619</td>\n",
       "      <td>0.117163</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.295846</td>\n",
       "      <td>0.009608</td>\n",
       "      <td>0.002260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>0.434368</td>\n",
       "      <td>0.056150</td>\n",
       "      <td>0.153684</td>\n",
       "      <td>0.064817</td>\n",
       "      <td>0.067846</td>\n",
       "      <td>0.116703</td>\n",
       "      <td>0.046071</td>\n",
       "      <td>0.128078</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.007511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043178</td>\n",
       "      <td>0.073139</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.162518</td>\n",
       "      <td>0.035587</td>\n",
       "      <td>0.133887</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.309887</td>\n",
       "      <td>0.013715</td>\n",
       "      <td>0.003532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>0.437271</td>\n",
       "      <td>0.069811</td>\n",
       "      <td>0.160625</td>\n",
       "      <td>0.071835</td>\n",
       "      <td>0.069414</td>\n",
       "      <td>0.124904</td>\n",
       "      <td>0.061045</td>\n",
       "      <td>0.139776</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>0.010933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>0.084120</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.183829</td>\n",
       "      <td>0.043923</td>\n",
       "      <td>0.140746</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>0.328735</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.005252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>0.430149</td>\n",
       "      <td>0.037013</td>\n",
       "      <td>0.137524</td>\n",
       "      <td>0.050697</td>\n",
       "      <td>0.058553</td>\n",
       "      <td>0.098130</td>\n",
       "      <td>0.028772</td>\n",
       "      <td>0.101471</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031396</td>\n",
       "      <td>0.054018</td>\n",
       "      <td>0.002812</td>\n",
       "      <td>0.132523</td>\n",
       "      <td>0.022520</td>\n",
       "      <td>0.111992</td>\n",
       "      <td>0.007637</td>\n",
       "      <td>0.283915</td>\n",
       "      <td>0.008035</td>\n",
       "      <td>0.001668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>0.429837</td>\n",
       "      <td>0.039329</td>\n",
       "      <td>0.137451</td>\n",
       "      <td>0.051850</td>\n",
       "      <td>0.058444</td>\n",
       "      <td>0.099194</td>\n",
       "      <td>0.030266</td>\n",
       "      <td>0.105677</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032449</td>\n",
       "      <td>0.055296</td>\n",
       "      <td>0.002937</td>\n",
       "      <td>0.135912</td>\n",
       "      <td>0.023822</td>\n",
       "      <td>0.115296</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.286241</td>\n",
       "      <td>0.008479</td>\n",
       "      <td>0.001709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>0.430686</td>\n",
       "      <td>0.042250</td>\n",
       "      <td>0.137730</td>\n",
       "      <td>0.052901</td>\n",
       "      <td>0.057957</td>\n",
       "      <td>0.100889</td>\n",
       "      <td>0.033553</td>\n",
       "      <td>0.110101</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.004520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033687</td>\n",
       "      <td>0.058582</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.140976</td>\n",
       "      <td>0.025871</td>\n",
       "      <td>0.117528</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>0.292412</td>\n",
       "      <td>0.009263</td>\n",
       "      <td>0.002005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10346</th>\n",
       "      <td>0.431496</td>\n",
       "      <td>0.041682</td>\n",
       "      <td>0.147843</td>\n",
       "      <td>0.058235</td>\n",
       "      <td>0.070405</td>\n",
       "      <td>0.108044</td>\n",
       "      <td>0.032268</td>\n",
       "      <td>0.113642</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.004312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038354</td>\n",
       "      <td>0.061189</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.139053</td>\n",
       "      <td>0.026955</td>\n",
       "      <td>0.124894</td>\n",
       "      <td>0.009532</td>\n",
       "      <td>0.287457</td>\n",
       "      <td>0.010383</td>\n",
       "      <td>0.002108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10347</th>\n",
       "      <td>0.432443</td>\n",
       "      <td>0.044906</td>\n",
       "      <td>0.155498</td>\n",
       "      <td>0.062489</td>\n",
       "      <td>0.078616</td>\n",
       "      <td>0.114856</td>\n",
       "      <td>0.034708</td>\n",
       "      <td>0.122181</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044371</td>\n",
       "      <td>0.067463</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.142949</td>\n",
       "      <td>0.030596</td>\n",
       "      <td>0.133608</td>\n",
       "      <td>0.010634</td>\n",
       "      <td>0.290155</td>\n",
       "      <td>0.012388</td>\n",
       "      <td>0.002583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10348</th>\n",
       "      <td>0.434036</td>\n",
       "      <td>0.050818</td>\n",
       "      <td>0.155699</td>\n",
       "      <td>0.065206</td>\n",
       "      <td>0.072825</td>\n",
       "      <td>0.116958</td>\n",
       "      <td>0.040716</td>\n",
       "      <td>0.123437</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.006278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043095</td>\n",
       "      <td>0.069878</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>0.154093</td>\n",
       "      <td>0.032781</td>\n",
       "      <td>0.133077</td>\n",
       "      <td>0.012254</td>\n",
       "      <td>0.301065</td>\n",
       "      <td>0.013022</td>\n",
       "      <td>0.003107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10349</th>\n",
       "      <td>0.432013</td>\n",
       "      <td>0.043003</td>\n",
       "      <td>0.150432</td>\n",
       "      <td>0.061727</td>\n",
       "      <td>0.073207</td>\n",
       "      <td>0.111806</td>\n",
       "      <td>0.033033</td>\n",
       "      <td>0.116252</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.004857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040373</td>\n",
       "      <td>0.062534</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.140107</td>\n",
       "      <td>0.027953</td>\n",
       "      <td>0.129114</td>\n",
       "      <td>0.010543</td>\n",
       "      <td>0.287180</td>\n",
       "      <td>0.011211</td>\n",
       "      <td>0.002267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10350</th>\n",
       "      <td>0.433181</td>\n",
       "      <td>0.048510</td>\n",
       "      <td>0.152226</td>\n",
       "      <td>0.063490</td>\n",
       "      <td>0.070499</td>\n",
       "      <td>0.113874</td>\n",
       "      <td>0.038332</td>\n",
       "      <td>0.119904</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040711</td>\n",
       "      <td>0.066179</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.150589</td>\n",
       "      <td>0.030677</td>\n",
       "      <td>0.130189</td>\n",
       "      <td>0.011389</td>\n",
       "      <td>0.297481</td>\n",
       "      <td>0.011989</td>\n",
       "      <td>0.002696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10351</th>\n",
       "      <td>0.431080</td>\n",
       "      <td>0.040209</td>\n",
       "      <td>0.143995</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>0.068014</td>\n",
       "      <td>0.105383</td>\n",
       "      <td>0.031412</td>\n",
       "      <td>0.111705</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036602</td>\n",
       "      <td>0.059257</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.136689</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>0.122333</td>\n",
       "      <td>0.009469</td>\n",
       "      <td>0.286060</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.001957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10352</th>\n",
       "      <td>0.430810</td>\n",
       "      <td>0.042592</td>\n",
       "      <td>0.143980</td>\n",
       "      <td>0.056320</td>\n",
       "      <td>0.063854</td>\n",
       "      <td>0.105304</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>0.111851</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036352</td>\n",
       "      <td>0.059514</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>0.140481</td>\n",
       "      <td>0.026434</td>\n",
       "      <td>0.122396</td>\n",
       "      <td>0.008592</td>\n",
       "      <td>0.288916</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.002022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10353</th>\n",
       "      <td>0.430298</td>\n",
       "      <td>0.038441</td>\n",
       "      <td>0.146128</td>\n",
       "      <td>0.057098</td>\n",
       "      <td>0.072457</td>\n",
       "      <td>0.106510</td>\n",
       "      <td>0.028914</td>\n",
       "      <td>0.112324</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038438</td>\n",
       "      <td>0.058705</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.131941</td>\n",
       "      <td>0.025390</td>\n",
       "      <td>0.125181</td>\n",
       "      <td>0.008828</td>\n",
       "      <td>0.279736</td>\n",
       "      <td>0.009952</td>\n",
       "      <td>0.001819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10354</th>\n",
       "      <td>0.428574</td>\n",
       "      <td>0.035402</td>\n",
       "      <td>0.134955</td>\n",
       "      <td>0.050762</td>\n",
       "      <td>0.061290</td>\n",
       "      <td>0.097548</td>\n",
       "      <td>0.026689</td>\n",
       "      <td>0.104926</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.003403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032693</td>\n",
       "      <td>0.052913</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.127058</td>\n",
       "      <td>0.022277</td>\n",
       "      <td>0.115886</td>\n",
       "      <td>0.007389</td>\n",
       "      <td>0.277075</td>\n",
       "      <td>0.008169</td>\n",
       "      <td>0.001450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10355</th>\n",
       "      <td>0.438892</td>\n",
       "      <td>0.085986</td>\n",
       "      <td>0.141415</td>\n",
       "      <td>0.059630</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.110087</td>\n",
       "      <td>0.085557</td>\n",
       "      <td>0.129051</td>\n",
       "      <td>0.006712</td>\n",
       "      <td>0.014516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033535</td>\n",
       "      <td>0.083295</td>\n",
       "      <td>0.007741</td>\n",
       "      <td>0.213689</td>\n",
       "      <td>0.044828</td>\n",
       "      <td>0.118412</td>\n",
       "      <td>0.015741</td>\n",
       "      <td>0.363380</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.006887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10356</th>\n",
       "      <td>0.431480</td>\n",
       "      <td>0.044275</td>\n",
       "      <td>0.140033</td>\n",
       "      <td>0.053957</td>\n",
       "      <td>0.056887</td>\n",
       "      <td>0.102050</td>\n",
       "      <td>0.035336</td>\n",
       "      <td>0.108817</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033103</td>\n",
       "      <td>0.059132</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.145870</td>\n",
       "      <td>0.026276</td>\n",
       "      <td>0.116494</td>\n",
       "      <td>0.008719</td>\n",
       "      <td>0.296643</td>\n",
       "      <td>0.009254</td>\n",
       "      <td>0.002136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10357 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0      0.435248  0.056095  0.160077  0.068470  0.075845  0.121998  0.046047   \n",
       "1      0.431858  0.050662  0.128042  0.046942  0.040974  0.093354  0.044119   \n",
       "2      0.429936  0.039634  0.138280  0.052903  0.060935  0.100455  0.030663   \n",
       "3      0.429860  0.039201  0.138427  0.052335  0.059751  0.099938  0.030040   \n",
       "4      0.434904  0.061005  0.144354  0.058547  0.052167  0.108814  0.053142   \n",
       "5      0.432274  0.050722  0.138208  0.053917  0.051736  0.102044  0.042046   \n",
       "6      0.430631  0.044036  0.137183  0.052785  0.055083  0.100297  0.034738   \n",
       "7      0.431717  0.041810  0.146072  0.055348  0.064299  0.105269  0.032730   \n",
       "8      0.433062  0.046025  0.145592  0.055576  0.061132  0.106047  0.038009   \n",
       "9      0.436490  0.060621  0.156116  0.064372  0.064286  0.117707  0.052771   \n",
       "10     0.432936  0.049771  0.146273  0.058225  0.060401  0.108433  0.040246   \n",
       "11     0.431748  0.041220  0.148251  0.057299  0.070271  0.107867  0.032217   \n",
       "12     0.431645  0.043346  0.150466  0.060802  0.071509  0.111092  0.032784   \n",
       "13     0.433727  0.053516  0.147114  0.059293  0.059856  0.109784  0.044460   \n",
       "14     0.431604  0.043182  0.153448  0.063144  0.077551  0.113932  0.032267   \n",
       "15     0.435097  0.063999  0.141487  0.054711  0.047515  0.106032  0.057685   \n",
       "16     0.435273  0.063456  0.154552  0.067550  0.066239  0.119246  0.053413   \n",
       "17     0.430049  0.040746  0.134843  0.050787  0.054432  0.097663  0.032009   \n",
       "18     0.435239  0.059838  0.154025  0.064883  0.065887  0.117132  0.050705   \n",
       "19     0.430839  0.040667  0.142612  0.054194  0.061094  0.102835  0.031195   \n",
       "20     0.433756  0.051259  0.159084  0.066979  0.078231  0.119709  0.040241   \n",
       "21     0.432867  0.047160  0.154916  0.062519  0.073569  0.114156  0.036670   \n",
       "22     0.438496  0.083501  0.149868  0.063636  0.049040  0.116312  0.079708   \n",
       "23     0.432655  0.046435  0.155723  0.064566  0.077802  0.116051  0.035712   \n",
       "24     0.432656  0.051009  0.142802  0.056659  0.056598  0.105937  0.041745   \n",
       "25     0.433489  0.057770  0.144930  0.060473  0.059606  0.110872  0.048632   \n",
       "26     0.429613  0.041336  0.133041  0.050046  0.052695  0.096649  0.032347   \n",
       "27     0.430025  0.039950  0.141805  0.055860  0.065414  0.103872  0.030192   \n",
       "28     0.435004  0.054341  0.159012  0.066150  0.075515  0.119828  0.044927   \n",
       "29     0.437162  0.057857  0.172369  0.074732  0.088967  0.130632  0.048004   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10327  0.433208  0.050479  0.151202  0.062301  0.067355  0.113265  0.040167   \n",
       "10328  0.433867  0.049614  0.149663  0.059294  0.062615  0.110163  0.040658   \n",
       "10329  0.430428  0.041213  0.138525  0.052239  0.058685  0.100294  0.032383   \n",
       "10330  0.432583  0.049688  0.148011  0.058424  0.062270  0.109245  0.039445   \n",
       "10331  0.430546  0.038780  0.141268  0.052991  0.064223  0.101760  0.030373   \n",
       "10332  0.431392  0.041822  0.145292  0.055143  0.064979  0.105189  0.032715   \n",
       "10333  0.429627  0.038024  0.138047  0.052492  0.061163  0.099857  0.029020   \n",
       "10334  0.436704  0.048575  0.166225  0.066709  0.079951  0.121322  0.040433   \n",
       "10335  0.433308  0.055586  0.150810  0.064437  0.066108  0.115178  0.044407   \n",
       "10336  0.436506  0.048446  0.168307  0.068535  0.085477  0.123407  0.039953   \n",
       "10337  0.430632  0.041806  0.140089  0.054069  0.060427  0.102143  0.032614   \n",
       "10338  0.430396  0.041406  0.139064  0.053413  0.059300  0.101214  0.032177   \n",
       "10339  0.430518  0.040860  0.145616  0.058281  0.070886  0.107411  0.030960   \n",
       "10340  0.432337  0.042609  0.143740  0.054449  0.062591  0.103760  0.034939   \n",
       "10341  0.434368  0.056150  0.153684  0.064817  0.067846  0.116703  0.046071   \n",
       "10342  0.437271  0.069811  0.160625  0.071835  0.069414  0.124904  0.061045   \n",
       "10343  0.430149  0.037013  0.137524  0.050697  0.058553  0.098130  0.028772   \n",
       "10344  0.429837  0.039329  0.137451  0.051850  0.058444  0.099194  0.030266   \n",
       "10345  0.430686  0.042250  0.137730  0.052901  0.057957  0.100889  0.033553   \n",
       "10346  0.431496  0.041682  0.147843  0.058235  0.070405  0.108044  0.032268   \n",
       "10347  0.432443  0.044906  0.155498  0.062489  0.078616  0.114856  0.034708   \n",
       "10348  0.434036  0.050818  0.155699  0.065206  0.072825  0.116958  0.040716   \n",
       "10349  0.432013  0.043003  0.150432  0.061727  0.073207  0.111806  0.033033   \n",
       "10350  0.433181  0.048510  0.152226  0.063490  0.070499  0.113874  0.038332   \n",
       "10351  0.431080  0.040209  0.143995  0.056584  0.068014  0.105383  0.031412   \n",
       "10352  0.430810  0.042592  0.143980  0.056320  0.063854  0.105304  0.032540   \n",
       "10353  0.430298  0.038441  0.146128  0.057098  0.072457  0.106510  0.028914   \n",
       "10354  0.428574  0.035402  0.134955  0.050762  0.061290  0.097548  0.026689   \n",
       "10355  0.438892  0.085986  0.141415  0.059630  0.039835  0.110087  0.085557   \n",
       "10356  0.431480  0.044275  0.140033  0.053957  0.056887  0.102050  0.035336   \n",
       "\n",
       "             7         8         9     ...           18        19        20  \\\n",
       "0      0.131524  0.003802  0.007955    ...     0.047340  0.076626  0.006314   \n",
       "1      0.106936  0.002390  0.005458    ...     0.027585  0.060010  0.003468   \n",
       "2      0.108412  0.001686  0.003892    ...     0.033643  0.056420  0.003088   \n",
       "3      0.106245  0.001659  0.003679    ...     0.033047  0.055610  0.002973   \n",
       "4      0.121740  0.003687  0.007941    ...     0.036010  0.071629  0.005306   \n",
       "5      0.113366  0.002470  0.005554    ...     0.032809  0.062744  0.003880   \n",
       "6      0.109370  0.001935  0.004440    ...     0.032656  0.058134  0.003299   \n",
       "7      0.109090  0.002084  0.004066    ...     0.035917  0.060343  0.003503   \n",
       "8      0.112523  0.002564  0.005055    ...     0.036042  0.064348  0.004086   \n",
       "9      0.126958  0.004366  0.008374    ...     0.042293  0.077666  0.006347   \n",
       "10     0.116596  0.002691  0.005701    ...     0.037271  0.065515  0.004347   \n",
       "11     0.113083  0.002127  0.004354    ...     0.038623  0.061980  0.003782   \n",
       "12     0.115586  0.002167  0.004626    ...     0.040110  0.062210  0.003925   \n",
       "13     0.120171  0.003023  0.006450    ...     0.037885  0.068534  0.004763   \n",
       "14     0.119077  0.002175  0.004746    ...     0.042807  0.063499  0.004110   \n",
       "15     0.123307  0.004159  0.008844    ...     0.035508  0.074899  0.005640   \n",
       "16     0.134421  0.003989  0.009150    ...     0.044185  0.077394  0.006392   \n",
       "17     0.105351  0.001724  0.003928    ...     0.031175  0.055463  0.002985   \n",
       "18     0.130138  0.003849  0.008271    ...     0.043022  0.076059  0.006049   \n",
       "19     0.106338  0.001858  0.003792    ...     0.034075  0.057311  0.003179   \n",
       "20     0.127892  0.003044  0.006383    ...     0.046456  0.071936  0.005326   \n",
       "21     0.119885  0.002548  0.005088    ...     0.042085  0.066816  0.004391   \n",
       "22     0.136735  0.006578  0.013890    ...     0.039516  0.087385  0.008202   \n",
       "23     0.122417  0.002505  0.005325    ...     0.043904  0.066955  0.004558   \n",
       "24     0.116440  0.002621  0.005782    ...     0.035548  0.064836  0.004199   \n",
       "25     0.129041  0.003282  0.008214    ...     0.040038  0.072050  0.005445   \n",
       "26     0.106215  0.001698  0.004035    ...     0.030937  0.055352  0.002961   \n",
       "27     0.110738  0.001718  0.003998    ...     0.035782  0.057170  0.003238   \n",
       "28     0.129886  0.003619  0.007365    ...     0.046380  0.075928  0.005982   \n",
       "29     0.135454  0.004511  0.008084    ...     0.053010  0.082648  0.007253   \n",
       "...         ...       ...       ...    ...          ...       ...       ...   \n",
       "10327  0.121417  0.002808  0.006057    ...     0.040854  0.067753  0.004741   \n",
       "10328  0.115204  0.002855  0.005528    ...     0.037574  0.066457  0.004453   \n",
       "10329  0.108522  0.001831  0.004060    ...     0.033397  0.057781  0.003199   \n",
       "10330  0.117968  0.002640  0.005507    ...     0.038551  0.066109  0.004304   \n",
       "10331  0.108401  0.001786  0.003826    ...     0.034825  0.057923  0.003216   \n",
       "10332  0.111249  0.002076  0.004250    ...     0.036722  0.060968  0.003586   \n",
       "10333  0.106060  0.001589  0.003612    ...     0.033169  0.054899  0.002931   \n",
       "10334  0.116914  0.003610  0.005508    ...     0.043949  0.072559  0.005421   \n",
       "10335  0.128017  0.003080  0.007305    ...     0.042398  0.070643  0.005266   \n",
       "10336  0.120394  0.003556  0.005607    ...     0.046312  0.073741  0.005578   \n",
       "10337  0.109707  0.001869  0.004195    ...     0.034203  0.058141  0.003305   \n",
       "10338  0.108665  0.001813  0.004085    ...     0.033595  0.057344  0.003204   \n",
       "10339  0.115245  0.001871  0.004369    ...     0.038782  0.059973  0.003600   \n",
       "10340  0.109567  0.002171  0.004325    ...     0.034611  0.061134  0.003604   \n",
       "10341  0.128078  0.003425  0.007511    ...     0.043178  0.073139  0.005626   \n",
       "10342  0.139776  0.005087  0.010933    ...     0.047088  0.084120  0.007722   \n",
       "10343  0.101471  0.001615  0.003350    ...     0.031396  0.054018  0.002812   \n",
       "10344  0.105677  0.001650  0.003676    ...     0.032449  0.055296  0.002937   \n",
       "10345  0.110101  0.001943  0.004520    ...     0.033687  0.058582  0.003416   \n",
       "10346  0.113642  0.002035  0.004312    ...     0.038354  0.061189  0.003688   \n",
       "10347  0.122181  0.002484  0.005152    ...     0.044371  0.067463  0.004518   \n",
       "10348  0.123437  0.003023  0.006278    ...     0.043095  0.069878  0.005118   \n",
       "10349  0.116252  0.002218  0.004857    ...     0.040373  0.062534  0.004087   \n",
       "10350  0.119904  0.002626  0.005621    ...     0.040711  0.066179  0.004547   \n",
       "10351  0.111705  0.001899  0.004228    ...     0.036602  0.059257  0.003511   \n",
       "10352  0.111851  0.001955  0.004286    ...     0.036352  0.059514  0.003475   \n",
       "10353  0.112324  0.001759  0.003899    ...     0.038438  0.058705  0.003387   \n",
       "10354  0.104926  0.001407  0.003403    ...     0.032693  0.052913  0.002736   \n",
       "10355  0.129051  0.006712  0.014516    ...     0.033535  0.083295  0.007741   \n",
       "10356  0.108817  0.002065  0.004435    ...     0.033103  0.059132  0.003427   \n",
       "\n",
       "             21        22        23        24        25        26        27  \n",
       "0      0.161534  0.037209  0.139064  0.014042  0.308225  0.015129  0.003911  \n",
       "1      0.160404  0.027471  0.105933  0.008125  0.316956  0.008367  0.002491  \n",
       "2      0.136009  0.024523  0.117712  0.008126  0.286155  0.008878  0.001747  \n",
       "3      0.135393  0.023953  0.116278  0.007722  0.285410  0.008611  0.001719  \n",
       "4      0.174064  0.035118  0.121988  0.011695  0.324707  0.012027  0.003812  \n",
       "5      0.157552  0.029179  0.116717  0.009260  0.309145  0.009851  0.002563  \n",
       "6      0.144678  0.026004  0.116429  0.008203  0.295423  0.009056  0.002003  \n",
       "7      0.140703  0.026173  0.119453  0.008742  0.290713  0.009673  0.002166  \n",
       "8      0.149204  0.028629  0.119266  0.009934  0.301247  0.010377  0.002662  \n",
       "9      0.172340  0.037800  0.130054  0.013914  0.321646  0.014152  0.004515  \n",
       "10     0.153972  0.030280  0.123475  0.010127  0.303652  0.011061  0.002779  \n",
       "11     0.138341  0.027015  0.124020  0.009510  0.287833  0.010424  0.002205  \n",
       "12     0.140597  0.027848  0.128521  0.009682  0.287275  0.010993  0.002228  \n",
       "13     0.160713  0.032453  0.124744  0.010989  0.310493  0.011727  0.003131  \n",
       "14     0.139214  0.028636  0.133140  0.010150  0.284677  0.011699  0.002238  \n",
       "15     0.179197  0.036986  0.119217  0.011088  0.332750  0.012110  0.004347  \n",
       "16     0.173774  0.039452  0.137062  0.014059  0.319705  0.014997  0.004118  \n",
       "17     0.139195  0.024106  0.113191  0.007731  0.290749  0.008350  0.001783  \n",
       "18     0.169405  0.037606  0.133286  0.013376  0.317339  0.014189  0.003987  \n",
       "19     0.138352  0.024741  0.117410  0.008076  0.287812  0.009014  0.001922  \n",
       "20     0.153311  0.034144  0.138198  0.012166  0.298970  0.013912  0.003150  \n",
       "21     0.148142  0.030589  0.130797  0.010468  0.294945  0.011978  0.002652  \n",
       "22     0.207062  0.047242  0.127955  0.015876  0.354746  0.015736  0.006838  \n",
       "23     0.145564  0.030811  0.134530  0.011087  0.291457  0.012500  0.002588  \n",
       "24     0.156770  0.030293  0.121299  0.009840  0.307031  0.010680  0.002716  \n",
       "25     0.165369  0.035697  0.130050  0.011941  0.314930  0.012996  0.003392  \n",
       "26     0.139846  0.024289  0.113205  0.007449  0.291516  0.008308  0.001758  \n",
       "27     0.135305  0.025079  0.122160  0.008498  0.283419  0.009445  0.001770  \n",
       "28     0.159672  0.036360  0.136609  0.013382  0.307551  0.014536  0.003754  \n",
       "29     0.164945  0.040153  0.145185  0.016439  0.309875  0.017210  0.004671  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "10327  0.153564  0.031771  0.129992  0.011071  0.301174  0.012208  0.002892  \n",
       "10328  0.154907  0.030347  0.122881  0.010704  0.304718  0.011187  0.002950  \n",
       "10329  0.139558  0.025243  0.116535  0.008091  0.290671  0.008961  0.001904  \n",
       "10330  0.153109  0.030634  0.125347  0.009624  0.301998  0.011256  0.002746  \n",
       "10331  0.134805  0.024773  0.117986  0.008402  0.285735  0.009116  0.001858  \n",
       "10332  0.139962  0.026622  0.120955  0.008759  0.290151  0.009898  0.002160  \n",
       "10333  0.132867  0.023494  0.116693  0.007822  0.282671  0.008560  0.001641  \n",
       "10334  0.153827  0.032197  0.129296  0.013704  0.302452  0.013139  0.003724  \n",
       "10335  0.160479  0.034656  0.134420  0.011750  0.306791  0.013289  0.003167  \n",
       "10336  0.152648  0.033018  0.133249  0.014154  0.300307  0.013801  0.003678  \n",
       "10337  0.140158  0.025610  0.118643  0.008478  0.290190  0.009263  0.001934  \n",
       "10338  0.139492  0.025174  0.117649  0.008220  0.289599  0.009037  0.001876  \n",
       "10339  0.136082  0.026621  0.126944  0.009264  0.283622  0.010381  0.001930  \n",
       "10340  0.143856  0.026619  0.117163  0.009549  0.295846  0.009608  0.002260  \n",
       "10341  0.162518  0.035587  0.133887  0.012588  0.309887  0.013715  0.003532  \n",
       "10342  0.183829  0.043923  0.140746  0.016871  0.328735  0.016992  0.005252  \n",
       "10343  0.132523  0.022520  0.111992  0.007637  0.283915  0.008035  0.001668  \n",
       "10344  0.135912  0.023822  0.115296  0.007663  0.286241  0.008479  0.001709  \n",
       "10345  0.140976  0.025871  0.117528  0.008614  0.292412  0.009263  0.002005  \n",
       "10346  0.139053  0.026955  0.124894  0.009532  0.287457  0.010383  0.002108  \n",
       "10347  0.142949  0.030596  0.133608  0.010634  0.290155  0.012388  0.002583  \n",
       "10348  0.154093  0.032781  0.133077  0.012254  0.301065  0.013022  0.003107  \n",
       "10349  0.140107  0.027953  0.129114  0.010543  0.287180  0.011211  0.002267  \n",
       "10350  0.150589  0.030677  0.130189  0.011389  0.297481  0.011989  0.002696  \n",
       "10351  0.136689  0.025847  0.122333  0.009469  0.286060  0.009851  0.001957  \n",
       "10352  0.140481  0.026434  0.122396  0.008592  0.288916  0.009836  0.002022  \n",
       "10353  0.131941  0.025390  0.125181  0.008828  0.279736  0.009952  0.001819  \n",
       "10354  0.127058  0.022277  0.115886  0.007389  0.277075  0.008169  0.001450  \n",
       "10355  0.213689  0.044828  0.118412  0.015741  0.363380  0.013892  0.006887  \n",
       "10356  0.145870  0.026276  0.116494  0.008719  0.296643  0.009254  0.002136  \n",
       "\n",
       "[10357 rows x 28 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(proba_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_proba_predictions.to_csv(\"Base_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
