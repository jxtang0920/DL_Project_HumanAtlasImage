{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import keras\n",
    "# import cv2\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"/projectnb/dl-course/jxtang/EC500_proj/train/\"\n",
    "test_path = \"/projectnb/dl-course/jxtang/EC500_proj/test/\"\n",
    "label_path = './train.csv'\n",
    "train_files = listdir(train_path)\n",
    "test_files = listdir(test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00070df0-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>16 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7 1 2 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a9596-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001838f8-bbca-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id   Target\n",
       "0  00070df0-bbc3-11e8-b2bc-ac1f6b6435d0     16 0\n",
       "1  000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0  7 1 2 0\n",
       "2  000a9596-bbc4-11e8-b2bc-ac1f6b6435d0        5\n",
       "3  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0        1\n",
       "4  001838f8-bbca-11e8-b2bc-ac1f6b6435d0       18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv(label_path)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_dict = {\n",
    "    0:  \"Nucleoplasm\",  \n",
    "    1:  \"Nuclear membrane\",   \n",
    "    2:  \"Nucleoli\",   \n",
    "    3:  \"Nucleoli fibrillar center\",   \n",
    "    4:  \"Nuclear speckles\",\n",
    "    5:  \"Nuclear bodies\",   \n",
    "    6:  \"Endoplasmic reticulum\",   \n",
    "    7:  \"Golgi apparatus\",   \n",
    "    8:  \"Peroxisomes\",   \n",
    "    9:  \"Endosomes\",   \n",
    "    10:  \"Lysosomes\",   \n",
    "    11:  \"Intermediate filaments\",   \n",
    "    12:  \"Actin filaments\",   \n",
    "    13:  \"Focal adhesion sites\",   \n",
    "    14:  \"Microtubules\",   \n",
    "    15:  \"Microtubule ends\",   \n",
    "    16:  \"Cytokinetic bridge\",   \n",
    "    17:  \"Mitotic spindle\",   \n",
    "    18:  \"Microtubule organizing center\",   \n",
    "    19:  \"Centrosome\",   \n",
    "    20:  \"Lipid droplets\",   \n",
    "    21:  \"Plasma membrane\",   \n",
    "    22:  \"Cell junctions\",   \n",
    "    23:  \"Mitochondria\",   \n",
    "    24:  \"Aggresome\",   \n",
    "    25:  \"Cytosol\",   \n",
    "    26:  \"Cytoplasmic bodies\",   \n",
    "    27:  \"Rods & rings\"\n",
    "}\n",
    "# reverse_names_dict = dict((v,k) for k,v in names_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_targets(row):\n",
    "    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n",
    "    for num in row.Target:\n",
    "        name = names_dict[int(num)]\n",
    "        row.loc[name] = 1\n",
    "    return row\n",
    "\n",
    "for key in names_dict.keys():\n",
    "    train_labels[names_dict[key]] = 0\n",
    "train_labels = train_labels.apply(fill_targets, axis=1)\n",
    "train_labels[\"number_of_targets\"] = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "for train_idx, test_idx in kf.split(train_labels.index.values):\n",
    "    partition = {}\n",
    "    partition[\"train\"] = train_labels.Id.values[train_idx]\n",
    "    partition[\"validation\"] = train_labels.Id.values[test_idx]\n",
    "#     X_train, X_test = train_labels.Id.values[train_idx], train_labels.Id.values[test_idx]\n",
    "#     y_train, y_test = train_labels.Id.values[train_idx], train_labels.Id.values[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': array(['00070df0-bbc3-11e8-b2bc-ac1f6b6435d0',\n",
       "        '000a6c98-bb9b-11e8-b2b9-ac1f6b6435d0',\n",
       "        '000a9596-bbc4-11e8-b2bc-ac1f6b6435d0', ...,\n",
       "        'ab33dff8-bba7-11e8-b2ba-ac1f6b6435d0',\n",
       "        'ab351f1c-bbb6-11e8-b2ba-ac1f6b6435d0',\n",
       "        'ab385b2e-bbba-11e8-b2ba-ac1f6b6435d0'], dtype=object),\n",
       " 'validation': array(['ab3978aa-bbbb-11e8-b2ba-ac1f6b6435d0',\n",
       "        'ab3b9258-bbab-11e8-b2ba-ac1f6b6435d0',\n",
       "        'ab3ca16e-bbc1-11e8-b2bb-ac1f6b6435d0', ...,\n",
       "        'fff189d8-bbab-11e8-b2ba-ac1f6b6435d0',\n",
       "        'fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0',\n",
       "        'fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0'], dtype=object)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelParameters(object):\n",
    "    path = train_path\n",
    "    num_classes=28\n",
    "    image_rows=512\n",
    "    image_cols=512\n",
    "    batch_size=50\n",
    "    n_channels=3\n",
    "    shuffle=False\n",
    "    scaled_row_dim = 256\n",
    "    scaled_col_dim = 256\n",
    "    n_epochs=300\n",
    "\n",
    "parameter = ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImagePreprocessor:\n",
    "    \n",
    "    def __init__(self, modelparameter):\n",
    "        self.parameter = modelparameter\n",
    "        self.path = self.parameter.path\n",
    "        self.scaled_row_dim = self.parameter.scaled_row_dim\n",
    "        self.scaled_col_dim = self.parameter.scaled_col_dim\n",
    "        self.n_channels = self.parameter.n_channels\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        image = self.resize(image)\n",
    "        image = self.reshape(image)\n",
    "        image = self.normalize(image)\n",
    "        return image\n",
    "    \n",
    "    def resize(self, image):\n",
    "        return resize(image, (self.scaled_row_dim, self.scaled_col_dim))\n",
    "    \n",
    "    def reshape(self, image):\n",
    "        return np.reshape(image, (image.shape[0], image.shape[1], self.n_channels))\n",
    "    \n",
    "    def normalize(self, image):\n",
    "#         image /= 255\n",
    "#         return image\n",
    "        return (image / 255.0 - 0.5) / 0.5\n",
    "            \n",
    "    def load_image(self, image_id):\n",
    "        image = np.zeros(shape=(512,512,4))\n",
    "        image[:,:,0] = imread(self.basepath + image_id + \"_green\" + \".png\")\n",
    "        image[:,:,1] = imread(self.basepath + image_id + \"_blue\" + \".png\")\n",
    "        image[:,:,2] = imread(self.basepath + image_id + \"_red\" + \".png\")\n",
    "        image[:,:,3] = imread(self.basepath + image_id + \"_yellow\" + \".png\")\n",
    "        return image[:,:,0:self.parameter.n_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessor = ImagePreprocessor(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor):\n",
    "        self.params = modelparameter\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dim = (self.params.scaled_row_dim, self.params.scaled_col_dim)\n",
    "        self.batch_size = self.params.batch_size\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.num_classes = self.params.num_classes\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.shuffle = self.params.shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier].drop(\n",
    "                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n",
    "        for i, identifier in enumerate(list_IDs_temp):\n",
    "            image = self.preprocessor.load_image(identifier)\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            X[i] = image\n",
    "            y[i] = self.get_targets_per_image(identifier)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PredictGenerator:\n",
    "    \n",
    "    def __init__(self, predict_Ids, imagepreprocessor, predict_path):\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.preprocessor.basepath = predict_path\n",
    "        self.identifiers = predict_Ids\n",
    "    \n",
    "    def predict(self, model):\n",
    "        y = np.empty(shape=(len(self.identifiers), self.preprocessor.parameter.num_classes))\n",
    "        for n in range(len(self.identifiers)):\n",
    "            image = self.preprocessor.load_image(self.identifiers[n])\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            image = image.reshape((1, *image.shape))\n",
    "            y[n] = model.predict(image)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.models import load_model\n",
    "\n",
    "class BaseLineModel:\n",
    "    \n",
    "    def __init__(self, modelparameter):\n",
    "        self.params = modelparameter\n",
    "        self.num_classes = self.params.num_classes\n",
    "        self.img_rows = self.params.scaled_row_dim\n",
    "        self.img_cols = self.params.scaled_col_dim\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.input_shape = (self.img_rows, self.img_cols, self.n_channels)\n",
    "        self.my_metrics = ['accuracy']\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n",
    "        self.model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(self.num_classes, activation='sigmoid'))\n",
    "\n",
    "    def compile_model(self):\n",
    "        self.model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=self.my_metrics)\n",
    "    \n",
    "    def set_generators(self, train_generator, validation_generator):\n",
    "        self.training_generator = train_generator\n",
    "        self.validation_generator = validation_generator\n",
    "    \n",
    "    def learn(self):\n",
    "        return self.model.fit_generator(generator=self.training_generator,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    epochs=self.params.n_epochs, \n",
    "                    steps_per_epoch=100,\n",
    "                    use_multiprocessing=True,\n",
    "                    validation_steps=50,\n",
    "                    workers=8)\n",
    "    \n",
    "    def score(self):\n",
    "        return self.model.evaluate_generator(generator=self.validation_generator,\n",
    "                                      use_multiprocessing=True, \n",
    "                                      workers=8)\n",
    "\n",
    "    def predict(self, predict_generator):\n",
    "        y = predict_generator.predict(self.model)\n",
    "        return y\n",
    "    \n",
    "    def save(self, modeloutputpath):\n",
    "        self.model.save(modeloutputpath)\n",
    "    \n",
    "    def load(self, modelinputpath):\n",
    "        self.model = load_model(modelinputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train: 20715\n",
      "Number of samples in validation: 10357\n"
     ]
    }
   ],
   "source": [
    "labels = train_labels\n",
    "print(\"Number of samples in train: {}\".format(len(partition[\"train\"])))\n",
    "print(\"Number of samples in validation: {}\".format(len(partition[\"validation\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels, parameter, preprocessor)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, parameter, preprocessor)\n",
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-124:\n",
      "Process ForkPoolWorker-117:\n",
      "Process ForkPoolWorker-120:\n",
      "Process ForkPoolWorker-123:\n",
      "Process ForkPoolWorker-118:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-119:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 371, in get_index\n",
      "    return ds[i]\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 40, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 28, in __data_generation\n",
      "    image = self.preprocessor.load_image(identifier)\n",
      "  File \"<ipython-input-109-20bfa20f1de1>\", line 31, in load_image\n",
      "    image[:,:,2] = imread(self.basepath + image_id + \"_red\" + \".png\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 36, in imread\n",
      "    im = Image.open(f)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/PIL/Image.py\", line 2618, in open\n",
      "    prefix = fp.read(16)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 371, in get_index\n",
      "    return ds[i]\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 40, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 28, in __data_generation\n",
      "    image = self.preprocessor.load_image(identifier)\n",
      "  File \"<ipython-input-109-20bfa20f1de1>\", line 29, in load_image\n",
      "    image[:,:,0] = imread(self.basepath + image_id + \"_green\" + \".png\")\n",
      "Process ForkPoolWorker-121:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 36, in imread\n",
      "    im = Image.open(f)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/PIL/Image.py\", line 2618, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 371, in get_index\n",
      "    return ds[i]\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 40, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 28, in __data_generation\n",
      "    image = self.preprocessor.load_image(identifier)\n",
      "  File \"<ipython-input-109-20bfa20f1de1>\", line 30, in load_image\n",
      "    image[:,:,1] = imread(self.basepath + image_id + \"_blue\" + \".png\")\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 36, in imread\n",
      "    im = Image.open(f)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/PIL/Image.py\", line 2618, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 371, in get_index\n",
      "    return ds[i]\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 40, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 31, in __data_generation\n",
      "    y[i] = self.get_targets_per_image(identifier)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 371, in get_index\n",
      "    return ds[i]\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 371, in get_index\n",
      "    return ds[i]\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 21, in get_targets_per_image\n",
      "    return self.labels.loc[self.labels.Id==identifier].drop(\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 40, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 40, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/pandas/core/ops.py\", line 1283, in wrapper\n",
      "    res = na_op(values, other)\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 29, in __data_generation\n",
      "    image = self.preprocessor.preprocess(image)\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 29, in __data_generation\n",
      "    image = self.preprocessor.preprocess(image)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-109-20bfa20f1de1>\", line 11, in preprocess\n",
      "    image = self.resize(image)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/pandas/core/ops.py\", line 1143, in na_op\n",
      "    result = _comp_method_OBJECT_ARRAY(op, x, y)\n",
      "  File \"<ipython-input-109-20bfa20f1de1>\", line 11, in preprocess\n",
      "    image = self.resize(image)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/pandas/core/ops.py\", line 1122, in _comp_method_OBJECT_ARRAY\n",
      "    result = libops.scalar_compare(x, y, op)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-109-20bfa20f1de1>\", line 17, in resize\n",
      "    return resize(image, (self.scaled_row_dim, self.scaled_col_dim))\n",
      "  File \"<ipython-input-109-20bfa20f1de1>\", line 17, in resize\n",
      "    return resize(image, (self.scaled_row_dim, self.scaled_col_dim))\n",
      "KeyboardInterrupt\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 135, in resize\n",
      "    preserve_range=preserve_range)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 135, in resize\n",
      "    preserve_range=preserve_range)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 819, in warp\n",
      "    _clip_warp_output(image, warped, order, mode, cval, clip)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 819, in warp\n",
      "    _clip_warp_output(image, warped, order, mode, cval, clip)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 570, in _clip_warp_output\n",
      "    min_val = input_image.min()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 371, in get_index\n",
      "    return ds[i]\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 571, in _clip_warp_output\n",
      "    max_val = input_image.max()\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 40, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/numpy/core/_methods.py\", line 29, in _amin\n",
      "    return umr_minimum(a, axis, None, out, keepdims)\n",
      "  File \"/share/pkg/tensorflow/r1.10/install/py3-gpu/lib/python3.6/site-packages/numpy/core/_methods.py\", line 26, in _amax\n",
      "    return umr_maximum(a, axis, None, out, keepdims)\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 31, in __data_generation\n",
      "    y[i] = self.get_targets_per_image(identifier)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 21, in get_targets_per_image\n",
      "    return self.labels.loc[self.labels.Id==identifier].drop(\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/pandas/core/ops.py\", line 1291, in wrapper\n",
      "    name=res_name, dtype='bool')\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/pandas/core/series.py\", line 277, in __init__\n",
      "    data = SingleBlockManager(data, index, fastpath=True)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/pandas/core/internals.py\", line 4677, in __init__\n",
      "    block = make_block(block, placement=slice(0, len(axis)), ndim=1)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/pandas/core/internals.py\", line 3199, in make_block\n",
      "    klass = get_block_type(values, dtype)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/pandas/core/internals.py\", line 3181, in get_block_type\n",
      "    cls = BoolBlock\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-122:\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 371, in get_index\n",
      "    return ds[i]\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 40, in __getitem__\n",
      "    X, y = self.__data_generation(list_IDs_temp)\n",
      "  File \"<ipython-input-111-f1c9b30f20f0>\", line 28, in __data_generation\n",
      "    image = self.preprocessor.load_image(identifier)\n",
      "  File \"<ipython-input-109-20bfa20f1de1>\", line 30, in load_image\n",
      "    image[:,:,1] = imread(self.basepath + image_id + \"_blue\" + \".png\")\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/_io.py\", line 61, in imread\n",
      "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/manage_plugins.py\", line 211, in call_plugin\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/io/_plugins/pil_plugin.py\", line 36, in imread\n",
      "    im = Image.open(f)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/PIL/Image.py\", line 2618, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-132:\n",
      "Process ForkPoolWorker-130:\n",
      "Process ForkPoolWorker-129:\n",
      "Process ForkPoolWorker-135:\n",
      "Process ForkPoolWorker-136:\n",
      "Process ForkPoolWorker-133:\n",
      "Process ForkPoolWorker-131:\n",
      "Process ForkPoolWorker-126:\n",
      "Process ForkPoolWorker-134:\n",
      "Process ForkPoolWorker-128:\n",
      "Process ForkPoolWorker-125:\n",
      "Process ForkPoolWorker-127:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2010\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-8fe276b23103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#model.save(\"baseline_model.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mproba_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-a0b8ee2eff3c>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     workers=8)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2087\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0menqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2089\u001b[0;31m                 \u001b[0menqueuer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joining pool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCLOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.2/install/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-146:\n",
      "Process ForkPoolWorker-137:\n",
      "Process ForkPoolWorker-144:\n",
      "Process ForkPoolWorker-143:\n",
      "Process ForkPoolWorker-138:\n",
      "Process ForkPoolWorker-139:\n",
      "Process ForkPoolWorker-145:\n",
      "Process ForkPoolWorker-142:\n",
      "Process ForkPoolWorker-141:\n",
      "Process ForkPoolWorker-140:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/share/pkg/python/3.6.2/install/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model = BaseLineModel(parameter)\n",
    "model.build_model()\n",
    "model.compile_model()\n",
    "model.set_generators(training_generator, validation_generator)\n",
    "history = model.learn()\n",
    "#model.save(\"baseline_model.h5\")\n",
    "proba_predictions = model.predict(predict_generator)\n",
    "baseline_proba_predictions = pd.DataFrame(proba_predictions, columns=train_labels.drop(\n",
    "    [\"Target\", \"number_of_targets\", \"Id\"], axis=1).columns)\n",
    "baseline_proba_predictions.to_csv(\"baseline_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nucleoplasm</th>\n",
       "      <th>Nuclear membrane</th>\n",
       "      <th>Nucleoli</th>\n",
       "      <th>Nucleoli fibrillar center</th>\n",
       "      <th>Nuclear speckles</th>\n",
       "      <th>Nuclear bodies</th>\n",
       "      <th>Endoplasmic reticulum</th>\n",
       "      <th>Golgi apparatus</th>\n",
       "      <th>Peroxisomes</th>\n",
       "      <th>Endosomes</th>\n",
       "      <th>...</th>\n",
       "      <th>Microtubule organizing center</th>\n",
       "      <th>Centrosome</th>\n",
       "      <th>Lipid droplets</th>\n",
       "      <th>Plasma membrane</th>\n",
       "      <th>Cell junctions</th>\n",
       "      <th>Mitochondria</th>\n",
       "      <th>Aggresome</th>\n",
       "      <th>Cytosol</th>\n",
       "      <th>Cytoplasmic bodies</th>\n",
       "      <th>Rods &amp; rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.376832</td>\n",
       "      <td>0.046780</td>\n",
       "      <td>0.061799</td>\n",
       "      <td>0.086671</td>\n",
       "      <td>0.022863</td>\n",
       "      <td>0.048857</td>\n",
       "      <td>0.143351</td>\n",
       "      <td>0.094772</td>\n",
       "      <td>0.017296</td>\n",
       "      <td>0.049911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031554</td>\n",
       "      <td>0.043320</td>\n",
       "      <td>0.073627</td>\n",
       "      <td>0.077283</td>\n",
       "      <td>0.047587</td>\n",
       "      <td>0.201696</td>\n",
       "      <td>0.029957</td>\n",
       "      <td>0.294167</td>\n",
       "      <td>0.037141</td>\n",
       "      <td>0.030636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.379009</td>\n",
       "      <td>0.055425</td>\n",
       "      <td>0.074850</td>\n",
       "      <td>0.100301</td>\n",
       "      <td>0.029612</td>\n",
       "      <td>0.062742</td>\n",
       "      <td>0.159723</td>\n",
       "      <td>0.110091</td>\n",
       "      <td>0.024523</td>\n",
       "      <td>0.063089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039895</td>\n",
       "      <td>0.051550</td>\n",
       "      <td>0.084751</td>\n",
       "      <td>0.087392</td>\n",
       "      <td>0.058937</td>\n",
       "      <td>0.214846</td>\n",
       "      <td>0.038274</td>\n",
       "      <td>0.310294</td>\n",
       "      <td>0.045202</td>\n",
       "      <td>0.039707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.350029</td>\n",
       "      <td>0.050695</td>\n",
       "      <td>0.064352</td>\n",
       "      <td>0.094351</td>\n",
       "      <td>0.025668</td>\n",
       "      <td>0.054158</td>\n",
       "      <td>0.139662</td>\n",
       "      <td>0.116092</td>\n",
       "      <td>0.022074</td>\n",
       "      <td>0.056161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039601</td>\n",
       "      <td>0.047033</td>\n",
       "      <td>0.074418</td>\n",
       "      <td>0.072260</td>\n",
       "      <td>0.050324</td>\n",
       "      <td>0.181006</td>\n",
       "      <td>0.033442</td>\n",
       "      <td>0.247697</td>\n",
       "      <td>0.037618</td>\n",
       "      <td>0.032566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.345599</td>\n",
       "      <td>0.053750</td>\n",
       "      <td>0.067764</td>\n",
       "      <td>0.101102</td>\n",
       "      <td>0.027440</td>\n",
       "      <td>0.056822</td>\n",
       "      <td>0.142196</td>\n",
       "      <td>0.125807</td>\n",
       "      <td>0.024467</td>\n",
       "      <td>0.059165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043356</td>\n",
       "      <td>0.049655</td>\n",
       "      <td>0.077290</td>\n",
       "      <td>0.073665</td>\n",
       "      <td>0.052880</td>\n",
       "      <td>0.180115</td>\n",
       "      <td>0.036528</td>\n",
       "      <td>0.237169</td>\n",
       "      <td>0.039492</td>\n",
       "      <td>0.034519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.393301</td>\n",
       "      <td>0.072534</td>\n",
       "      <td>0.091997</td>\n",
       "      <td>0.120062</td>\n",
       "      <td>0.040748</td>\n",
       "      <td>0.075797</td>\n",
       "      <td>0.180900</td>\n",
       "      <td>0.130547</td>\n",
       "      <td>0.031854</td>\n",
       "      <td>0.076789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051189</td>\n",
       "      <td>0.066905</td>\n",
       "      <td>0.102592</td>\n",
       "      <td>0.108494</td>\n",
       "      <td>0.070740</td>\n",
       "      <td>0.226998</td>\n",
       "      <td>0.048502</td>\n",
       "      <td>0.313710</td>\n",
       "      <td>0.058137</td>\n",
       "      <td>0.049274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.364394</td>\n",
       "      <td>0.056697</td>\n",
       "      <td>0.072970</td>\n",
       "      <td>0.101948</td>\n",
       "      <td>0.030591</td>\n",
       "      <td>0.061433</td>\n",
       "      <td>0.151446</td>\n",
       "      <td>0.117273</td>\n",
       "      <td>0.025211</td>\n",
       "      <td>0.063906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042773</td>\n",
       "      <td>0.053786</td>\n",
       "      <td>0.080611</td>\n",
       "      <td>0.084409</td>\n",
       "      <td>0.056152</td>\n",
       "      <td>0.192301</td>\n",
       "      <td>0.037529</td>\n",
       "      <td>0.269018</td>\n",
       "      <td>0.044169</td>\n",
       "      <td>0.037816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.353359</td>\n",
       "      <td>0.053774</td>\n",
       "      <td>0.068572</td>\n",
       "      <td>0.101403</td>\n",
       "      <td>0.028175</td>\n",
       "      <td>0.058123</td>\n",
       "      <td>0.144633</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.024471</td>\n",
       "      <td>0.060497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042393</td>\n",
       "      <td>0.050409</td>\n",
       "      <td>0.078038</td>\n",
       "      <td>0.075983</td>\n",
       "      <td>0.053265</td>\n",
       "      <td>0.185011</td>\n",
       "      <td>0.036509</td>\n",
       "      <td>0.250154</td>\n",
       "      <td>0.040464</td>\n",
       "      <td>0.035083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.364654</td>\n",
       "      <td>0.054069</td>\n",
       "      <td>0.070941</td>\n",
       "      <td>0.095332</td>\n",
       "      <td>0.028438</td>\n",
       "      <td>0.058604</td>\n",
       "      <td>0.150786</td>\n",
       "      <td>0.110215</td>\n",
       "      <td>0.022985</td>\n",
       "      <td>0.060748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040020</td>\n",
       "      <td>0.050830</td>\n",
       "      <td>0.080434</td>\n",
       "      <td>0.083146</td>\n",
       "      <td>0.054772</td>\n",
       "      <td>0.194876</td>\n",
       "      <td>0.035535</td>\n",
       "      <td>0.277231</td>\n",
       "      <td>0.042854</td>\n",
       "      <td>0.036196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.357120</td>\n",
       "      <td>0.058195</td>\n",
       "      <td>0.073742</td>\n",
       "      <td>0.103477</td>\n",
       "      <td>0.030490</td>\n",
       "      <td>0.061876</td>\n",
       "      <td>0.152743</td>\n",
       "      <td>0.125696</td>\n",
       "      <td>0.026007</td>\n",
       "      <td>0.063538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045736</td>\n",
       "      <td>0.053361</td>\n",
       "      <td>0.083630</td>\n",
       "      <td>0.081620</td>\n",
       "      <td>0.057115</td>\n",
       "      <td>0.193401</td>\n",
       "      <td>0.039230</td>\n",
       "      <td>0.261425</td>\n",
       "      <td>0.044124</td>\n",
       "      <td>0.037734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.389428</td>\n",
       "      <td>0.069030</td>\n",
       "      <td>0.088897</td>\n",
       "      <td>0.114983</td>\n",
       "      <td>0.040422</td>\n",
       "      <td>0.076736</td>\n",
       "      <td>0.177646</td>\n",
       "      <td>0.129077</td>\n",
       "      <td>0.031054</td>\n",
       "      <td>0.076533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052691</td>\n",
       "      <td>0.065328</td>\n",
       "      <td>0.099702</td>\n",
       "      <td>0.108332</td>\n",
       "      <td>0.068749</td>\n",
       "      <td>0.228085</td>\n",
       "      <td>0.048842</td>\n",
       "      <td>0.320969</td>\n",
       "      <td>0.059272</td>\n",
       "      <td>0.049704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.365489</td>\n",
       "      <td>0.059020</td>\n",
       "      <td>0.075827</td>\n",
       "      <td>0.105090</td>\n",
       "      <td>0.031971</td>\n",
       "      <td>0.064003</td>\n",
       "      <td>0.156690</td>\n",
       "      <td>0.124651</td>\n",
       "      <td>0.026860</td>\n",
       "      <td>0.064528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045748</td>\n",
       "      <td>0.055796</td>\n",
       "      <td>0.085112</td>\n",
       "      <td>0.085105</td>\n",
       "      <td>0.059329</td>\n",
       "      <td>0.198728</td>\n",
       "      <td>0.040632</td>\n",
       "      <td>0.273289</td>\n",
       "      <td>0.046041</td>\n",
       "      <td>0.040255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.350528</td>\n",
       "      <td>0.053493</td>\n",
       "      <td>0.068314</td>\n",
       "      <td>0.098879</td>\n",
       "      <td>0.027613</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>0.141496</td>\n",
       "      <td>0.120681</td>\n",
       "      <td>0.023618</td>\n",
       "      <td>0.058576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041772</td>\n",
       "      <td>0.049696</td>\n",
       "      <td>0.076195</td>\n",
       "      <td>0.075644</td>\n",
       "      <td>0.052886</td>\n",
       "      <td>0.182380</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>0.249338</td>\n",
       "      <td>0.039789</td>\n",
       "      <td>0.034190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.346932</td>\n",
       "      <td>0.050748</td>\n",
       "      <td>0.064673</td>\n",
       "      <td>0.096447</td>\n",
       "      <td>0.025746</td>\n",
       "      <td>0.054237</td>\n",
       "      <td>0.138805</td>\n",
       "      <td>0.118739</td>\n",
       "      <td>0.022521</td>\n",
       "      <td>0.055876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040259</td>\n",
       "      <td>0.046803</td>\n",
       "      <td>0.073883</td>\n",
       "      <td>0.071750</td>\n",
       "      <td>0.050548</td>\n",
       "      <td>0.178848</td>\n",
       "      <td>0.034097</td>\n",
       "      <td>0.239425</td>\n",
       "      <td>0.037326</td>\n",
       "      <td>0.032480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.380974</td>\n",
       "      <td>0.058812</td>\n",
       "      <td>0.075905</td>\n",
       "      <td>0.096048</td>\n",
       "      <td>0.030343</td>\n",
       "      <td>0.058735</td>\n",
       "      <td>0.158162</td>\n",
       "      <td>0.107385</td>\n",
       "      <td>0.023230</td>\n",
       "      <td>0.062627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040008</td>\n",
       "      <td>0.052941</td>\n",
       "      <td>0.085188</td>\n",
       "      <td>0.090954</td>\n",
       "      <td>0.057350</td>\n",
       "      <td>0.204092</td>\n",
       "      <td>0.037261</td>\n",
       "      <td>0.301040</td>\n",
       "      <td>0.046354</td>\n",
       "      <td>0.036565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.344289</td>\n",
       "      <td>0.052012</td>\n",
       "      <td>0.065930</td>\n",
       "      <td>0.098438</td>\n",
       "      <td>0.026460</td>\n",
       "      <td>0.055356</td>\n",
       "      <td>0.140009</td>\n",
       "      <td>0.122516</td>\n",
       "      <td>0.023350</td>\n",
       "      <td>0.057659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041858</td>\n",
       "      <td>0.048231</td>\n",
       "      <td>0.075524</td>\n",
       "      <td>0.071963</td>\n",
       "      <td>0.051094</td>\n",
       "      <td>0.178207</td>\n",
       "      <td>0.035082</td>\n",
       "      <td>0.236767</td>\n",
       "      <td>0.038306</td>\n",
       "      <td>0.033256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.411877</td>\n",
       "      <td>0.074387</td>\n",
       "      <td>0.094974</td>\n",
       "      <td>0.117614</td>\n",
       "      <td>0.044468</td>\n",
       "      <td>0.079714</td>\n",
       "      <td>0.185950</td>\n",
       "      <td>0.122803</td>\n",
       "      <td>0.033965</td>\n",
       "      <td>0.080244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053184</td>\n",
       "      <td>0.068702</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.114618</td>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.241362</td>\n",
       "      <td>0.051826</td>\n",
       "      <td>0.343999</td>\n",
       "      <td>0.062043</td>\n",
       "      <td>0.051574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.373442</td>\n",
       "      <td>0.055862</td>\n",
       "      <td>0.071152</td>\n",
       "      <td>0.097447</td>\n",
       "      <td>0.030703</td>\n",
       "      <td>0.063149</td>\n",
       "      <td>0.152576</td>\n",
       "      <td>0.108904</td>\n",
       "      <td>0.023534</td>\n",
       "      <td>0.064260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039539</td>\n",
       "      <td>0.052540</td>\n",
       "      <td>0.079939</td>\n",
       "      <td>0.089717</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.198189</td>\n",
       "      <td>0.036859</td>\n",
       "      <td>0.301756</td>\n",
       "      <td>0.045788</td>\n",
       "      <td>0.039982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.351406</td>\n",
       "      <td>0.054867</td>\n",
       "      <td>0.069771</td>\n",
       "      <td>0.101101</td>\n",
       "      <td>0.028345</td>\n",
       "      <td>0.057790</td>\n",
       "      <td>0.144910</td>\n",
       "      <td>0.124585</td>\n",
       "      <td>0.025142</td>\n",
       "      <td>0.060987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044363</td>\n",
       "      <td>0.051643</td>\n",
       "      <td>0.079571</td>\n",
       "      <td>0.075404</td>\n",
       "      <td>0.054415</td>\n",
       "      <td>0.185788</td>\n",
       "      <td>0.037152</td>\n",
       "      <td>0.245107</td>\n",
       "      <td>0.040822</td>\n",
       "      <td>0.035423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.369818</td>\n",
       "      <td>0.070721</td>\n",
       "      <td>0.088602</td>\n",
       "      <td>0.122060</td>\n",
       "      <td>0.039686</td>\n",
       "      <td>0.075180</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>0.142107</td>\n",
       "      <td>0.035107</td>\n",
       "      <td>0.077513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056360</td>\n",
       "      <td>0.066859</td>\n",
       "      <td>0.096932</td>\n",
       "      <td>0.098839</td>\n",
       "      <td>0.072290</td>\n",
       "      <td>0.209661</td>\n",
       "      <td>0.049867</td>\n",
       "      <td>0.277919</td>\n",
       "      <td>0.056545</td>\n",
       "      <td>0.049223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.350725</td>\n",
       "      <td>0.055015</td>\n",
       "      <td>0.069528</td>\n",
       "      <td>0.102157</td>\n",
       "      <td>0.028482</td>\n",
       "      <td>0.058573</td>\n",
       "      <td>0.145737</td>\n",
       "      <td>0.126049</td>\n",
       "      <td>0.024965</td>\n",
       "      <td>0.060378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044355</td>\n",
       "      <td>0.051341</td>\n",
       "      <td>0.079453</td>\n",
       "      <td>0.076575</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>0.185180</td>\n",
       "      <td>0.037301</td>\n",
       "      <td>0.243903</td>\n",
       "      <td>0.040851</td>\n",
       "      <td>0.035646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.366063</td>\n",
       "      <td>0.049510</td>\n",
       "      <td>0.063644</td>\n",
       "      <td>0.091368</td>\n",
       "      <td>0.025047</td>\n",
       "      <td>0.053287</td>\n",
       "      <td>0.144425</td>\n",
       "      <td>0.107516</td>\n",
       "      <td>0.020014</td>\n",
       "      <td>0.053671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035773</td>\n",
       "      <td>0.045514</td>\n",
       "      <td>0.072627</td>\n",
       "      <td>0.078245</td>\n",
       "      <td>0.049752</td>\n",
       "      <td>0.189759</td>\n",
       "      <td>0.031933</td>\n",
       "      <td>0.271735</td>\n",
       "      <td>0.038275</td>\n",
       "      <td>0.033056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.355557</td>\n",
       "      <td>0.055645</td>\n",
       "      <td>0.070182</td>\n",
       "      <td>0.102934</td>\n",
       "      <td>0.029029</td>\n",
       "      <td>0.059127</td>\n",
       "      <td>0.146426</td>\n",
       "      <td>0.124256</td>\n",
       "      <td>0.025209</td>\n",
       "      <td>0.061009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043831</td>\n",
       "      <td>0.051814</td>\n",
       "      <td>0.079821</td>\n",
       "      <td>0.078180</td>\n",
       "      <td>0.055190</td>\n",
       "      <td>0.187673</td>\n",
       "      <td>0.037880</td>\n",
       "      <td>0.251378</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>0.036387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.419109</td>\n",
       "      <td>0.084419</td>\n",
       "      <td>0.105448</td>\n",
       "      <td>0.127725</td>\n",
       "      <td>0.050348</td>\n",
       "      <td>0.087105</td>\n",
       "      <td>0.202513</td>\n",
       "      <td>0.135277</td>\n",
       "      <td>0.039638</td>\n",
       "      <td>0.090507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059161</td>\n",
       "      <td>0.079257</td>\n",
       "      <td>0.115558</td>\n",
       "      <td>0.129168</td>\n",
       "      <td>0.085231</td>\n",
       "      <td>0.250504</td>\n",
       "      <td>0.056858</td>\n",
       "      <td>0.353541</td>\n",
       "      <td>0.072414</td>\n",
       "      <td>0.060922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.350228</td>\n",
       "      <td>0.052016</td>\n",
       "      <td>0.066535</td>\n",
       "      <td>0.097026</td>\n",
       "      <td>0.026636</td>\n",
       "      <td>0.055618</td>\n",
       "      <td>0.142040</td>\n",
       "      <td>0.119142</td>\n",
       "      <td>0.022807</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041058</td>\n",
       "      <td>0.048332</td>\n",
       "      <td>0.076475</td>\n",
       "      <td>0.073970</td>\n",
       "      <td>0.051329</td>\n",
       "      <td>0.182947</td>\n",
       "      <td>0.034972</td>\n",
       "      <td>0.246795</td>\n",
       "      <td>0.038797</td>\n",
       "      <td>0.033528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.360735</td>\n",
       "      <td>0.060097</td>\n",
       "      <td>0.075969</td>\n",
       "      <td>0.106068</td>\n",
       "      <td>0.032114</td>\n",
       "      <td>0.064403</td>\n",
       "      <td>0.156174</td>\n",
       "      <td>0.128629</td>\n",
       "      <td>0.027518</td>\n",
       "      <td>0.065972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047646</td>\n",
       "      <td>0.055806</td>\n",
       "      <td>0.086082</td>\n",
       "      <td>0.085365</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>0.198664</td>\n",
       "      <td>0.040885</td>\n",
       "      <td>0.266808</td>\n",
       "      <td>0.046630</td>\n",
       "      <td>0.040404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.367964</td>\n",
       "      <td>0.051387</td>\n",
       "      <td>0.066884</td>\n",
       "      <td>0.094618</td>\n",
       "      <td>0.026770</td>\n",
       "      <td>0.055814</td>\n",
       "      <td>0.146107</td>\n",
       "      <td>0.108602</td>\n",
       "      <td>0.021126</td>\n",
       "      <td>0.056796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038107</td>\n",
       "      <td>0.049051</td>\n",
       "      <td>0.076851</td>\n",
       "      <td>0.079816</td>\n",
       "      <td>0.051605</td>\n",
       "      <td>0.195505</td>\n",
       "      <td>0.033888</td>\n",
       "      <td>0.277962</td>\n",
       "      <td>0.040162</td>\n",
       "      <td>0.033968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.363116</td>\n",
       "      <td>0.051565</td>\n",
       "      <td>0.064908</td>\n",
       "      <td>0.094610</td>\n",
       "      <td>0.026784</td>\n",
       "      <td>0.055059</td>\n",
       "      <td>0.140915</td>\n",
       "      <td>0.113027</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.056703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039114</td>\n",
       "      <td>0.047648</td>\n",
       "      <td>0.075827</td>\n",
       "      <td>0.077002</td>\n",
       "      <td>0.050681</td>\n",
       "      <td>0.187232</td>\n",
       "      <td>0.034241</td>\n",
       "      <td>0.257908</td>\n",
       "      <td>0.038382</td>\n",
       "      <td>0.033737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.345441</td>\n",
       "      <td>0.052266</td>\n",
       "      <td>0.066357</td>\n",
       "      <td>0.099036</td>\n",
       "      <td>0.026674</td>\n",
       "      <td>0.055745</td>\n",
       "      <td>0.141035</td>\n",
       "      <td>0.122927</td>\n",
       "      <td>0.023521</td>\n",
       "      <td>0.058133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.048635</td>\n",
       "      <td>0.075757</td>\n",
       "      <td>0.072704</td>\n",
       "      <td>0.051415</td>\n",
       "      <td>0.178526</td>\n",
       "      <td>0.035460</td>\n",
       "      <td>0.237836</td>\n",
       "      <td>0.038708</td>\n",
       "      <td>0.033685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.363522</td>\n",
       "      <td>0.049375</td>\n",
       "      <td>0.065560</td>\n",
       "      <td>0.087663</td>\n",
       "      <td>0.025708</td>\n",
       "      <td>0.054063</td>\n",
       "      <td>0.144267</td>\n",
       "      <td>0.103015</td>\n",
       "      <td>0.019860</td>\n",
       "      <td>0.054389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035541</td>\n",
       "      <td>0.045778</td>\n",
       "      <td>0.074046</td>\n",
       "      <td>0.077531</td>\n",
       "      <td>0.049376</td>\n",
       "      <td>0.189814</td>\n",
       "      <td>0.031392</td>\n",
       "      <td>0.280886</td>\n",
       "      <td>0.038653</td>\n",
       "      <td>0.032006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.360221</td>\n",
       "      <td>0.059413</td>\n",
       "      <td>0.075645</td>\n",
       "      <td>0.106302</td>\n",
       "      <td>0.031632</td>\n",
       "      <td>0.063862</td>\n",
       "      <td>0.155193</td>\n",
       "      <td>0.126842</td>\n",
       "      <td>0.027032</td>\n",
       "      <td>0.065837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046682</td>\n",
       "      <td>0.055123</td>\n",
       "      <td>0.086023</td>\n",
       "      <td>0.085686</td>\n",
       "      <td>0.058992</td>\n",
       "      <td>0.197959</td>\n",
       "      <td>0.040591</td>\n",
       "      <td>0.266147</td>\n",
       "      <td>0.046159</td>\n",
       "      <td>0.039713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10327</th>\n",
       "      <td>0.367068</td>\n",
       "      <td>0.060127</td>\n",
       "      <td>0.076274</td>\n",
       "      <td>0.105031</td>\n",
       "      <td>0.032378</td>\n",
       "      <td>0.064217</td>\n",
       "      <td>0.156685</td>\n",
       "      <td>0.124251</td>\n",
       "      <td>0.027705</td>\n",
       "      <td>0.065219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046165</td>\n",
       "      <td>0.055873</td>\n",
       "      <td>0.086208</td>\n",
       "      <td>0.087094</td>\n",
       "      <td>0.062228</td>\n",
       "      <td>0.199037</td>\n",
       "      <td>0.040919</td>\n",
       "      <td>0.275932</td>\n",
       "      <td>0.046745</td>\n",
       "      <td>0.040172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10328</th>\n",
       "      <td>0.364093</td>\n",
       "      <td>0.062170</td>\n",
       "      <td>0.077329</td>\n",
       "      <td>0.109151</td>\n",
       "      <td>0.032748</td>\n",
       "      <td>0.065510</td>\n",
       "      <td>0.158821</td>\n",
       "      <td>0.129790</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.067075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047890</td>\n",
       "      <td>0.056737</td>\n",
       "      <td>0.088046</td>\n",
       "      <td>0.088980</td>\n",
       "      <td>0.059789</td>\n",
       "      <td>0.202542</td>\n",
       "      <td>0.041517</td>\n",
       "      <td>0.268485</td>\n",
       "      <td>0.047557</td>\n",
       "      <td>0.041321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10329</th>\n",
       "      <td>0.349296</td>\n",
       "      <td>0.053255</td>\n",
       "      <td>0.067982</td>\n",
       "      <td>0.100818</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.057786</td>\n",
       "      <td>0.145995</td>\n",
       "      <td>0.123899</td>\n",
       "      <td>0.024398</td>\n",
       "      <td>0.059238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042718</td>\n",
       "      <td>0.049121</td>\n",
       "      <td>0.077197</td>\n",
       "      <td>0.075419</td>\n",
       "      <td>0.053155</td>\n",
       "      <td>0.182308</td>\n",
       "      <td>0.036727</td>\n",
       "      <td>0.244835</td>\n",
       "      <td>0.040456</td>\n",
       "      <td>0.035270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10330</th>\n",
       "      <td>0.360612</td>\n",
       "      <td>0.061211</td>\n",
       "      <td>0.076360</td>\n",
       "      <td>0.108762</td>\n",
       "      <td>0.032408</td>\n",
       "      <td>0.065237</td>\n",
       "      <td>0.154745</td>\n",
       "      <td>0.130620</td>\n",
       "      <td>0.027675</td>\n",
       "      <td>0.067184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048269</td>\n",
       "      <td>0.057532</td>\n",
       "      <td>0.087245</td>\n",
       "      <td>0.086143</td>\n",
       "      <td>0.060373</td>\n",
       "      <td>0.199136</td>\n",
       "      <td>0.041135</td>\n",
       "      <td>0.264660</td>\n",
       "      <td>0.046324</td>\n",
       "      <td>0.040735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10331</th>\n",
       "      <td>0.364564</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.065356</td>\n",
       "      <td>0.088373</td>\n",
       "      <td>0.026097</td>\n",
       "      <td>0.054665</td>\n",
       "      <td>0.146381</td>\n",
       "      <td>0.104019</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.056004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037413</td>\n",
       "      <td>0.046543</td>\n",
       "      <td>0.075702</td>\n",
       "      <td>0.077737</td>\n",
       "      <td>0.048355</td>\n",
       "      <td>0.193705</td>\n",
       "      <td>0.032067</td>\n",
       "      <td>0.280083</td>\n",
       "      <td>0.039765</td>\n",
       "      <td>0.032234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10332</th>\n",
       "      <td>0.354820</td>\n",
       "      <td>0.057204</td>\n",
       "      <td>0.072384</td>\n",
       "      <td>0.104579</td>\n",
       "      <td>0.030118</td>\n",
       "      <td>0.061547</td>\n",
       "      <td>0.151864</td>\n",
       "      <td>0.127847</td>\n",
       "      <td>0.025582</td>\n",
       "      <td>0.063634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046018</td>\n",
       "      <td>0.053857</td>\n",
       "      <td>0.082575</td>\n",
       "      <td>0.080278</td>\n",
       "      <td>0.055444</td>\n",
       "      <td>0.190571</td>\n",
       "      <td>0.038635</td>\n",
       "      <td>0.255272</td>\n",
       "      <td>0.043669</td>\n",
       "      <td>0.037375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10333</th>\n",
       "      <td>0.350740</td>\n",
       "      <td>0.054326</td>\n",
       "      <td>0.068901</td>\n",
       "      <td>0.102328</td>\n",
       "      <td>0.028309</td>\n",
       "      <td>0.058590</td>\n",
       "      <td>0.145786</td>\n",
       "      <td>0.123521</td>\n",
       "      <td>0.024065</td>\n",
       "      <td>0.060644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043493</td>\n",
       "      <td>0.050290</td>\n",
       "      <td>0.078750</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.052314</td>\n",
       "      <td>0.184907</td>\n",
       "      <td>0.037342</td>\n",
       "      <td>0.248744</td>\n",
       "      <td>0.041084</td>\n",
       "      <td>0.034968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10334</th>\n",
       "      <td>0.382638</td>\n",
       "      <td>0.066380</td>\n",
       "      <td>0.085020</td>\n",
       "      <td>0.109866</td>\n",
       "      <td>0.038788</td>\n",
       "      <td>0.072120</td>\n",
       "      <td>0.170322</td>\n",
       "      <td>0.120810</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>0.071543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048579</td>\n",
       "      <td>0.060856</td>\n",
       "      <td>0.095357</td>\n",
       "      <td>0.098504</td>\n",
       "      <td>0.066875</td>\n",
       "      <td>0.213461</td>\n",
       "      <td>0.045156</td>\n",
       "      <td>0.299545</td>\n",
       "      <td>0.054155</td>\n",
       "      <td>0.045559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10335</th>\n",
       "      <td>0.365989</td>\n",
       "      <td>0.060236</td>\n",
       "      <td>0.075637</td>\n",
       "      <td>0.105494</td>\n",
       "      <td>0.032189</td>\n",
       "      <td>0.063801</td>\n",
       "      <td>0.156655</td>\n",
       "      <td>0.126619</td>\n",
       "      <td>0.026696</td>\n",
       "      <td>0.065170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046129</td>\n",
       "      <td>0.055451</td>\n",
       "      <td>0.085807</td>\n",
       "      <td>0.086317</td>\n",
       "      <td>0.058653</td>\n",
       "      <td>0.196654</td>\n",
       "      <td>0.040518</td>\n",
       "      <td>0.269475</td>\n",
       "      <td>0.045936</td>\n",
       "      <td>0.039698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10336</th>\n",
       "      <td>0.372244</td>\n",
       "      <td>0.055503</td>\n",
       "      <td>0.073247</td>\n",
       "      <td>0.099770</td>\n",
       "      <td>0.030298</td>\n",
       "      <td>0.062903</td>\n",
       "      <td>0.158644</td>\n",
       "      <td>0.111521</td>\n",
       "      <td>0.023395</td>\n",
       "      <td>0.062150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040467</td>\n",
       "      <td>0.052110</td>\n",
       "      <td>0.082008</td>\n",
       "      <td>0.084750</td>\n",
       "      <td>0.054178</td>\n",
       "      <td>0.199879</td>\n",
       "      <td>0.037066</td>\n",
       "      <td>0.290542</td>\n",
       "      <td>0.043887</td>\n",
       "      <td>0.037356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10337</th>\n",
       "      <td>0.350999</td>\n",
       "      <td>0.055539</td>\n",
       "      <td>0.070395</td>\n",
       "      <td>0.103595</td>\n",
       "      <td>0.029086</td>\n",
       "      <td>0.059859</td>\n",
       "      <td>0.146753</td>\n",
       "      <td>0.126234</td>\n",
       "      <td>0.025618</td>\n",
       "      <td>0.062059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044774</td>\n",
       "      <td>0.052164</td>\n",
       "      <td>0.080056</td>\n",
       "      <td>0.077359</td>\n",
       "      <td>0.055083</td>\n",
       "      <td>0.185534</td>\n",
       "      <td>0.038071</td>\n",
       "      <td>0.246365</td>\n",
       "      <td>0.041690</td>\n",
       "      <td>0.036610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10338</th>\n",
       "      <td>0.348066</td>\n",
       "      <td>0.053346</td>\n",
       "      <td>0.067343</td>\n",
       "      <td>0.098378</td>\n",
       "      <td>0.027445</td>\n",
       "      <td>0.057423</td>\n",
       "      <td>0.141817</td>\n",
       "      <td>0.122198</td>\n",
       "      <td>0.023704</td>\n",
       "      <td>0.059563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042613</td>\n",
       "      <td>0.049485</td>\n",
       "      <td>0.077390</td>\n",
       "      <td>0.074884</td>\n",
       "      <td>0.052248</td>\n",
       "      <td>0.183005</td>\n",
       "      <td>0.035816</td>\n",
       "      <td>0.245316</td>\n",
       "      <td>0.039532</td>\n",
       "      <td>0.034953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10339</th>\n",
       "      <td>0.344754</td>\n",
       "      <td>0.053699</td>\n",
       "      <td>0.067776</td>\n",
       "      <td>0.101112</td>\n",
       "      <td>0.027574</td>\n",
       "      <td>0.057245</td>\n",
       "      <td>0.142302</td>\n",
       "      <td>0.126085</td>\n",
       "      <td>0.024485</td>\n",
       "      <td>0.059409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043509</td>\n",
       "      <td>0.049774</td>\n",
       "      <td>0.077161</td>\n",
       "      <td>0.073894</td>\n",
       "      <td>0.052820</td>\n",
       "      <td>0.179420</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.236862</td>\n",
       "      <td>0.039634</td>\n",
       "      <td>0.034688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>0.363428</td>\n",
       "      <td>0.056013</td>\n",
       "      <td>0.073272</td>\n",
       "      <td>0.099829</td>\n",
       "      <td>0.031019</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.156089</td>\n",
       "      <td>0.115433</td>\n",
       "      <td>0.024923</td>\n",
       "      <td>0.063060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043389</td>\n",
       "      <td>0.053241</td>\n",
       "      <td>0.083601</td>\n",
       "      <td>0.083023</td>\n",
       "      <td>0.054466</td>\n",
       "      <td>0.198563</td>\n",
       "      <td>0.037296</td>\n",
       "      <td>0.277539</td>\n",
       "      <td>0.046303</td>\n",
       "      <td>0.037051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>0.375472</td>\n",
       "      <td>0.053693</td>\n",
       "      <td>0.071919</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.028479</td>\n",
       "      <td>0.056591</td>\n",
       "      <td>0.155023</td>\n",
       "      <td>0.109346</td>\n",
       "      <td>0.022621</td>\n",
       "      <td>0.057936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038922</td>\n",
       "      <td>0.048638</td>\n",
       "      <td>0.081346</td>\n",
       "      <td>0.083294</td>\n",
       "      <td>0.053679</td>\n",
       "      <td>0.199001</td>\n",
       "      <td>0.036478</td>\n",
       "      <td>0.285791</td>\n",
       "      <td>0.042826</td>\n",
       "      <td>0.034850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>0.383121</td>\n",
       "      <td>0.070524</td>\n",
       "      <td>0.090160</td>\n",
       "      <td>0.118256</td>\n",
       "      <td>0.041307</td>\n",
       "      <td>0.075870</td>\n",
       "      <td>0.178966</td>\n",
       "      <td>0.134794</td>\n",
       "      <td>0.033079</td>\n",
       "      <td>0.075268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054619</td>\n",
       "      <td>0.066664</td>\n",
       "      <td>0.100851</td>\n",
       "      <td>0.104150</td>\n",
       "      <td>0.070698</td>\n",
       "      <td>0.222030</td>\n",
       "      <td>0.049413</td>\n",
       "      <td>0.304199</td>\n",
       "      <td>0.059816</td>\n",
       "      <td>0.048824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>0.358807</td>\n",
       "      <td>0.056523</td>\n",
       "      <td>0.072651</td>\n",
       "      <td>0.099440</td>\n",
       "      <td>0.029741</td>\n",
       "      <td>0.060186</td>\n",
       "      <td>0.152044</td>\n",
       "      <td>0.119680</td>\n",
       "      <td>0.025016</td>\n",
       "      <td>0.061408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043619</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>0.082140</td>\n",
       "      <td>0.083116</td>\n",
       "      <td>0.056430</td>\n",
       "      <td>0.196752</td>\n",
       "      <td>0.037587</td>\n",
       "      <td>0.268611</td>\n",
       "      <td>0.044510</td>\n",
       "      <td>0.037251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>0.345541</td>\n",
       "      <td>0.052687</td>\n",
       "      <td>0.066576</td>\n",
       "      <td>0.099405</td>\n",
       "      <td>0.026883</td>\n",
       "      <td>0.055962</td>\n",
       "      <td>0.140960</td>\n",
       "      <td>0.123841</td>\n",
       "      <td>0.023743</td>\n",
       "      <td>0.058269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042381</td>\n",
       "      <td>0.048728</td>\n",
       "      <td>0.076103</td>\n",
       "      <td>0.072734</td>\n",
       "      <td>0.051609</td>\n",
       "      <td>0.178745</td>\n",
       "      <td>0.035682</td>\n",
       "      <td>0.237223</td>\n",
       "      <td>0.038715</td>\n",
       "      <td>0.033782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>0.356105</td>\n",
       "      <td>0.051216</td>\n",
       "      <td>0.066191</td>\n",
       "      <td>0.094323</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>0.055115</td>\n",
       "      <td>0.144122</td>\n",
       "      <td>0.113186</td>\n",
       "      <td>0.021767</td>\n",
       "      <td>0.056636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038739</td>\n",
       "      <td>0.047075</td>\n",
       "      <td>0.075612</td>\n",
       "      <td>0.074661</td>\n",
       "      <td>0.050013</td>\n",
       "      <td>0.183614</td>\n",
       "      <td>0.033689</td>\n",
       "      <td>0.257364</td>\n",
       "      <td>0.038625</td>\n",
       "      <td>0.032746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10346</th>\n",
       "      <td>0.346132</td>\n",
       "      <td>0.053931</td>\n",
       "      <td>0.067906</td>\n",
       "      <td>0.100159</td>\n",
       "      <td>0.027594</td>\n",
       "      <td>0.057277</td>\n",
       "      <td>0.142866</td>\n",
       "      <td>0.125361</td>\n",
       "      <td>0.024267</td>\n",
       "      <td>0.059239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043318</td>\n",
       "      <td>0.049435</td>\n",
       "      <td>0.077688</td>\n",
       "      <td>0.074299</td>\n",
       "      <td>0.052748</td>\n",
       "      <td>0.181529</td>\n",
       "      <td>0.036464</td>\n",
       "      <td>0.241181</td>\n",
       "      <td>0.039799</td>\n",
       "      <td>0.034712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10347</th>\n",
       "      <td>0.347055</td>\n",
       "      <td>0.053099</td>\n",
       "      <td>0.067236</td>\n",
       "      <td>0.099541</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.056255</td>\n",
       "      <td>0.143168</td>\n",
       "      <td>0.123312</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.058443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042408</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>0.077097</td>\n",
       "      <td>0.073864</td>\n",
       "      <td>0.052056</td>\n",
       "      <td>0.181388</td>\n",
       "      <td>0.035822</td>\n",
       "      <td>0.241384</td>\n",
       "      <td>0.039455</td>\n",
       "      <td>0.033925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10348</th>\n",
       "      <td>0.358726</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>0.074617</td>\n",
       "      <td>0.106728</td>\n",
       "      <td>0.031232</td>\n",
       "      <td>0.062381</td>\n",
       "      <td>0.153574</td>\n",
       "      <td>0.128857</td>\n",
       "      <td>0.026918</td>\n",
       "      <td>0.065095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.054734</td>\n",
       "      <td>0.084658</td>\n",
       "      <td>0.082378</td>\n",
       "      <td>0.057442</td>\n",
       "      <td>0.192978</td>\n",
       "      <td>0.040486</td>\n",
       "      <td>0.258071</td>\n",
       "      <td>0.044954</td>\n",
       "      <td>0.038869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10349</th>\n",
       "      <td>0.350203</td>\n",
       "      <td>0.053495</td>\n",
       "      <td>0.067849</td>\n",
       "      <td>0.099697</td>\n",
       "      <td>0.027544</td>\n",
       "      <td>0.056855</td>\n",
       "      <td>0.143528</td>\n",
       "      <td>0.122350</td>\n",
       "      <td>0.024031</td>\n",
       "      <td>0.059043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042575</td>\n",
       "      <td>0.049427</td>\n",
       "      <td>0.078086</td>\n",
       "      <td>0.074892</td>\n",
       "      <td>0.052721</td>\n",
       "      <td>0.183757</td>\n",
       "      <td>0.036327</td>\n",
       "      <td>0.245253</td>\n",
       "      <td>0.039925</td>\n",
       "      <td>0.034570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10350</th>\n",
       "      <td>0.350932</td>\n",
       "      <td>0.053508</td>\n",
       "      <td>0.068685</td>\n",
       "      <td>0.100062</td>\n",
       "      <td>0.027880</td>\n",
       "      <td>0.057687</td>\n",
       "      <td>0.147156</td>\n",
       "      <td>0.122605</td>\n",
       "      <td>0.024078</td>\n",
       "      <td>0.059605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042930</td>\n",
       "      <td>0.050663</td>\n",
       "      <td>0.078106</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.053032</td>\n",
       "      <td>0.185045</td>\n",
       "      <td>0.035841</td>\n",
       "      <td>0.248719</td>\n",
       "      <td>0.041125</td>\n",
       "      <td>0.034899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10351</th>\n",
       "      <td>0.357061</td>\n",
       "      <td>0.053129</td>\n",
       "      <td>0.067339</td>\n",
       "      <td>0.098973</td>\n",
       "      <td>0.027603</td>\n",
       "      <td>0.057211</td>\n",
       "      <td>0.144241</td>\n",
       "      <td>0.119595</td>\n",
       "      <td>0.022980</td>\n",
       "      <td>0.058937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041143</td>\n",
       "      <td>0.049928</td>\n",
       "      <td>0.076575</td>\n",
       "      <td>0.076969</td>\n",
       "      <td>0.051462</td>\n",
       "      <td>0.184648</td>\n",
       "      <td>0.035596</td>\n",
       "      <td>0.253380</td>\n",
       "      <td>0.039656</td>\n",
       "      <td>0.034866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10352</th>\n",
       "      <td>0.345859</td>\n",
       "      <td>0.051751</td>\n",
       "      <td>0.065920</td>\n",
       "      <td>0.097044</td>\n",
       "      <td>0.026372</td>\n",
       "      <td>0.055260</td>\n",
       "      <td>0.140638</td>\n",
       "      <td>0.120491</td>\n",
       "      <td>0.023154</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041385</td>\n",
       "      <td>0.047826</td>\n",
       "      <td>0.075369</td>\n",
       "      <td>0.072273</td>\n",
       "      <td>0.051173</td>\n",
       "      <td>0.179841</td>\n",
       "      <td>0.034851</td>\n",
       "      <td>0.242434</td>\n",
       "      <td>0.038577</td>\n",
       "      <td>0.033225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10353</th>\n",
       "      <td>0.344485</td>\n",
       "      <td>0.053726</td>\n",
       "      <td>0.067722</td>\n",
       "      <td>0.100783</td>\n",
       "      <td>0.027545</td>\n",
       "      <td>0.057072</td>\n",
       "      <td>0.141979</td>\n",
       "      <td>0.126052</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.059235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043502</td>\n",
       "      <td>0.049593</td>\n",
       "      <td>0.077350</td>\n",
       "      <td>0.073420</td>\n",
       "      <td>0.052536</td>\n",
       "      <td>0.179119</td>\n",
       "      <td>0.036418</td>\n",
       "      <td>0.236215</td>\n",
       "      <td>0.039359</td>\n",
       "      <td>0.034318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10354</th>\n",
       "      <td>0.348553</td>\n",
       "      <td>0.051414</td>\n",
       "      <td>0.065887</td>\n",
       "      <td>0.096764</td>\n",
       "      <td>0.026152</td>\n",
       "      <td>0.054858</td>\n",
       "      <td>0.141326</td>\n",
       "      <td>0.119091</td>\n",
       "      <td>0.022658</td>\n",
       "      <td>0.057003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040569</td>\n",
       "      <td>0.047341</td>\n",
       "      <td>0.075667</td>\n",
       "      <td>0.072696</td>\n",
       "      <td>0.050657</td>\n",
       "      <td>0.180077</td>\n",
       "      <td>0.034638</td>\n",
       "      <td>0.242694</td>\n",
       "      <td>0.038069</td>\n",
       "      <td>0.032842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10355</th>\n",
       "      <td>0.442797</td>\n",
       "      <td>0.089626</td>\n",
       "      <td>0.118596</td>\n",
       "      <td>0.126195</td>\n",
       "      <td>0.058901</td>\n",
       "      <td>0.098770</td>\n",
       "      <td>0.224137</td>\n",
       "      <td>0.120869</td>\n",
       "      <td>0.040681</td>\n",
       "      <td>0.100374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060326</td>\n",
       "      <td>0.088018</td>\n",
       "      <td>0.128947</td>\n",
       "      <td>0.154988</td>\n",
       "      <td>0.094236</td>\n",
       "      <td>0.274464</td>\n",
       "      <td>0.064098</td>\n",
       "      <td>0.414894</td>\n",
       "      <td>0.087967</td>\n",
       "      <td>0.070674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10356</th>\n",
       "      <td>0.353390</td>\n",
       "      <td>0.055971</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.103027</td>\n",
       "      <td>0.029294</td>\n",
       "      <td>0.059643</td>\n",
       "      <td>0.149101</td>\n",
       "      <td>0.124936</td>\n",
       "      <td>0.025066</td>\n",
       "      <td>0.062301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044267</td>\n",
       "      <td>0.052154</td>\n",
       "      <td>0.080761</td>\n",
       "      <td>0.080050</td>\n",
       "      <td>0.054657</td>\n",
       "      <td>0.187942</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>0.252940</td>\n",
       "      <td>0.042884</td>\n",
       "      <td>0.036133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10357 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nucleoplasm  Nuclear membrane  Nucleoli  Nucleoli fibrillar center  \\\n",
       "0         0.376832          0.046780  0.061799                   0.086671   \n",
       "1         0.379009          0.055425  0.074850                   0.100301   \n",
       "2         0.350029          0.050695  0.064352                   0.094351   \n",
       "3         0.345599          0.053750  0.067764                   0.101102   \n",
       "4         0.393301          0.072534  0.091997                   0.120062   \n",
       "5         0.364394          0.056697  0.072970                   0.101948   \n",
       "6         0.353359          0.053774  0.068572                   0.101403   \n",
       "7         0.364654          0.054069  0.070941                   0.095332   \n",
       "8         0.357120          0.058195  0.073742                   0.103477   \n",
       "9         0.389428          0.069030  0.088897                   0.114983   \n",
       "10        0.365489          0.059020  0.075827                   0.105090   \n",
       "11        0.350528          0.053493  0.068314                   0.098879   \n",
       "12        0.346932          0.050748  0.064673                   0.096447   \n",
       "13        0.380974          0.058812  0.075905                   0.096048   \n",
       "14        0.344289          0.052012  0.065930                   0.098438   \n",
       "15        0.411877          0.074387  0.094974                   0.117614   \n",
       "16        0.373442          0.055862  0.071152                   0.097447   \n",
       "17        0.351406          0.054867  0.069771                   0.101101   \n",
       "18        0.369818          0.070721  0.088602                   0.122060   \n",
       "19        0.350725          0.055015  0.069528                   0.102157   \n",
       "20        0.366063          0.049510  0.063644                   0.091368   \n",
       "21        0.355557          0.055645  0.070182                   0.102934   \n",
       "22        0.419109          0.084419  0.105448                   0.127725   \n",
       "23        0.350228          0.052016  0.066535                   0.097026   \n",
       "24        0.360735          0.060097  0.075969                   0.106068   \n",
       "25        0.367964          0.051387  0.066884                   0.094618   \n",
       "26        0.363116          0.051565  0.064908                   0.094610   \n",
       "27        0.345441          0.052266  0.066357                   0.099036   \n",
       "28        0.363522          0.049375  0.065560                   0.087663   \n",
       "29        0.360221          0.059413  0.075645                   0.106302   \n",
       "...            ...               ...       ...                        ...   \n",
       "10327     0.367068          0.060127  0.076274                   0.105031   \n",
       "10328     0.364093          0.062170  0.077329                   0.109151   \n",
       "10329     0.349296          0.053255  0.067982                   0.100818   \n",
       "10330     0.360612          0.061211  0.076360                   0.108762   \n",
       "10331     0.364564          0.050004  0.065356                   0.088373   \n",
       "10332     0.354820          0.057204  0.072384                   0.104579   \n",
       "10333     0.350740          0.054326  0.068901                   0.102328   \n",
       "10334     0.382638          0.066380  0.085020                   0.109866   \n",
       "10335     0.365989          0.060236  0.075637                   0.105494   \n",
       "10336     0.372244          0.055503  0.073247                   0.099770   \n",
       "10337     0.350999          0.055539  0.070395                   0.103595   \n",
       "10338     0.348066          0.053346  0.067343                   0.098378   \n",
       "10339     0.344754          0.053699  0.067776                   0.101112   \n",
       "10340     0.363428          0.056013  0.073272                   0.099829   \n",
       "10341     0.375472          0.053693  0.071919                   0.094340   \n",
       "10342     0.383121          0.070524  0.090160                   0.118256   \n",
       "10343     0.358807          0.056523  0.072651                   0.099440   \n",
       "10344     0.345541          0.052687  0.066576                   0.099405   \n",
       "10345     0.356105          0.051216  0.066191                   0.094323   \n",
       "10346     0.346132          0.053931  0.067906                   0.100159   \n",
       "10347     0.347055          0.053099  0.067236                   0.099541   \n",
       "10348     0.358726          0.059261  0.074617                   0.106728   \n",
       "10349     0.350203          0.053495  0.067849                   0.099697   \n",
       "10350     0.350932          0.053508  0.068685                   0.100062   \n",
       "10351     0.357061          0.053129  0.067339                   0.098973   \n",
       "10352     0.345859          0.051751  0.065920                   0.097044   \n",
       "10353     0.344485          0.053726  0.067722                   0.100783   \n",
       "10354     0.348553          0.051414  0.065887                   0.096764   \n",
       "10355     0.442797          0.089626  0.118596                   0.126195   \n",
       "10356     0.353390          0.055971  0.071425                   0.103027   \n",
       "\n",
       "       Nuclear speckles  Nuclear bodies  Endoplasmic reticulum  \\\n",
       "0              0.022863        0.048857               0.143351   \n",
       "1              0.029612        0.062742               0.159723   \n",
       "2              0.025668        0.054158               0.139662   \n",
       "3              0.027440        0.056822               0.142196   \n",
       "4              0.040748        0.075797               0.180900   \n",
       "5              0.030591        0.061433               0.151446   \n",
       "6              0.028175        0.058123               0.144633   \n",
       "7              0.028438        0.058604               0.150786   \n",
       "8              0.030490        0.061876               0.152743   \n",
       "9              0.040422        0.076736               0.177646   \n",
       "10             0.031971        0.064003               0.156690   \n",
       "11             0.027613        0.056584               0.141496   \n",
       "12             0.025746        0.054237               0.138805   \n",
       "13             0.030343        0.058735               0.158162   \n",
       "14             0.026460        0.055356               0.140009   \n",
       "15             0.044468        0.079714               0.185950   \n",
       "16             0.030703        0.063149               0.152576   \n",
       "17             0.028345        0.057790               0.144910   \n",
       "18             0.039686        0.075180               0.170800   \n",
       "19             0.028482        0.058573               0.145737   \n",
       "20             0.025047        0.053287               0.144425   \n",
       "21             0.029029        0.059127               0.146426   \n",
       "22             0.050348        0.087105               0.202513   \n",
       "23             0.026636        0.055618               0.142040   \n",
       "24             0.032114        0.064403               0.156174   \n",
       "25             0.026770        0.055814               0.146107   \n",
       "26             0.026784        0.055059               0.140915   \n",
       "27             0.026674        0.055745               0.141035   \n",
       "28             0.025708        0.054063               0.144267   \n",
       "29             0.031632        0.063862               0.155193   \n",
       "...                 ...             ...                    ...   \n",
       "10327          0.032378        0.064217               0.156685   \n",
       "10328          0.032748        0.065510               0.158821   \n",
       "10329          0.027653        0.057786               0.145995   \n",
       "10330          0.032408        0.065237               0.154745   \n",
       "10331          0.026097        0.054665               0.146381   \n",
       "10332          0.030118        0.061547               0.151864   \n",
       "10333          0.028309        0.058590               0.145786   \n",
       "10334          0.038788        0.072120               0.170322   \n",
       "10335          0.032189        0.063801               0.156655   \n",
       "10336          0.030298        0.062903               0.158644   \n",
       "10337          0.029086        0.059859               0.146753   \n",
       "10338          0.027445        0.057423               0.141817   \n",
       "10339          0.027574        0.057245               0.142302   \n",
       "10340          0.031019        0.061404               0.156089   \n",
       "10341          0.028479        0.056591               0.155023   \n",
       "10342          0.041307        0.075870               0.178966   \n",
       "10343          0.029741        0.060186               0.152044   \n",
       "10344          0.026883        0.055962               0.140960   \n",
       "10345          0.026446        0.055115               0.144122   \n",
       "10346          0.027594        0.057277               0.142866   \n",
       "10347          0.027030        0.056255               0.143168   \n",
       "10348          0.031232        0.062381               0.153574   \n",
       "10349          0.027544        0.056855               0.143528   \n",
       "10350          0.027880        0.057687               0.147156   \n",
       "10351          0.027603        0.057211               0.144241   \n",
       "10352          0.026372        0.055260               0.140638   \n",
       "10353          0.027545        0.057072               0.141979   \n",
       "10354          0.026152        0.054858               0.141326   \n",
       "10355          0.058901        0.098770               0.224137   \n",
       "10356          0.029294        0.059643               0.149101   \n",
       "\n",
       "       Golgi apparatus  Peroxisomes  Endosomes      ...       \\\n",
       "0             0.094772     0.017296   0.049911      ...        \n",
       "1             0.110091     0.024523   0.063089      ...        \n",
       "2             0.116092     0.022074   0.056161      ...        \n",
       "3             0.125807     0.024467   0.059165      ...        \n",
       "4             0.130547     0.031854   0.076789      ...        \n",
       "5             0.117273     0.025211   0.063906      ...        \n",
       "6             0.120700     0.024471   0.060497      ...        \n",
       "7             0.110215     0.022985   0.060748      ...        \n",
       "8             0.125696     0.026007   0.063538      ...        \n",
       "9             0.129077     0.031054   0.076533      ...        \n",
       "10            0.124651     0.026860   0.064528      ...        \n",
       "11            0.120681     0.023618   0.058576      ...        \n",
       "12            0.118739     0.022521   0.055876      ...        \n",
       "13            0.107385     0.023230   0.062627      ...        \n",
       "14            0.122516     0.023350   0.057659      ...        \n",
       "15            0.122803     0.033965   0.080244      ...        \n",
       "16            0.108904     0.023534   0.064260      ...        \n",
       "17            0.124585     0.025142   0.060987      ...        \n",
       "18            0.142107     0.035107   0.077513      ...        \n",
       "19            0.126049     0.024965   0.060378      ...        \n",
       "20            0.107516     0.020014   0.053671      ...        \n",
       "21            0.124256     0.025209   0.061009      ...        \n",
       "22            0.135277     0.039638   0.090507      ...        \n",
       "23            0.119142     0.022807   0.057373      ...        \n",
       "24            0.128629     0.027518   0.065972      ...        \n",
       "25            0.108602     0.021126   0.056796      ...        \n",
       "26            0.113027     0.022001   0.056703      ...        \n",
       "27            0.122927     0.023521   0.058133      ...        \n",
       "28            0.103015     0.019860   0.054389      ...        \n",
       "29            0.126842     0.027032   0.065837      ...        \n",
       "...                ...          ...        ...      ...        \n",
       "10327         0.124251     0.027705   0.065219      ...        \n",
       "10328         0.129790     0.027380   0.067075      ...        \n",
       "10329         0.123899     0.024398   0.059238      ...        \n",
       "10330         0.130620     0.027675   0.067184      ...        \n",
       "10331         0.104019     0.020002   0.056004      ...        \n",
       "10332         0.127847     0.025582   0.063634      ...        \n",
       "10333         0.123521     0.024065   0.060644      ...        \n",
       "10334         0.120810     0.030713   0.071543      ...        \n",
       "10335         0.126619     0.026696   0.065170      ...        \n",
       "10336         0.111521     0.023395   0.062150      ...        \n",
       "10337         0.126234     0.025618   0.062059      ...        \n",
       "10338         0.122198     0.023704   0.059563      ...        \n",
       "10339         0.126085     0.024485   0.059409      ...        \n",
       "10340         0.115433     0.024923   0.063060      ...        \n",
       "10341         0.109346     0.022621   0.057936      ...        \n",
       "10342         0.134794     0.033079   0.075268      ...        \n",
       "10343         0.119680     0.025016   0.061408      ...        \n",
       "10344         0.123841     0.023743   0.058269      ...        \n",
       "10345         0.113186     0.021767   0.056636      ...        \n",
       "10346         0.125361     0.024267   0.059239      ...        \n",
       "10347         0.123312     0.023720   0.058443      ...        \n",
       "10348         0.128857     0.026918   0.065095      ...        \n",
       "10349         0.122350     0.024031   0.059043      ...        \n",
       "10350         0.122605     0.024078   0.059605      ...        \n",
       "10351         0.119595     0.022980   0.058937      ...        \n",
       "10352         0.120491     0.023154   0.057504      ...        \n",
       "10353         0.126052     0.024400   0.059235      ...        \n",
       "10354         0.119091     0.022658   0.057003      ...        \n",
       "10355         0.120869     0.040681   0.100374      ...        \n",
       "10356         0.124936     0.025066   0.062301      ...        \n",
       "\n",
       "       Microtubule organizing center  Centrosome  Lipid droplets  \\\n",
       "0                           0.031554    0.043320        0.073627   \n",
       "1                           0.039895    0.051550        0.084751   \n",
       "2                           0.039601    0.047033        0.074418   \n",
       "3                           0.043356    0.049655        0.077290   \n",
       "4                           0.051189    0.066905        0.102592   \n",
       "5                           0.042773    0.053786        0.080611   \n",
       "6                           0.042393    0.050409        0.078038   \n",
       "7                           0.040020    0.050830        0.080434   \n",
       "8                           0.045736    0.053361        0.083630   \n",
       "9                           0.052691    0.065328        0.099702   \n",
       "10                          0.045748    0.055796        0.085112   \n",
       "11                          0.041772    0.049696        0.076195   \n",
       "12                          0.040259    0.046803        0.073883   \n",
       "13                          0.040008    0.052941        0.085188   \n",
       "14                          0.041858    0.048231        0.075524   \n",
       "15                          0.053184    0.068702        0.106000   \n",
       "16                          0.039539    0.052540        0.079939   \n",
       "17                          0.044363    0.051643        0.079571   \n",
       "18                          0.056360    0.066859        0.096932   \n",
       "19                          0.044355    0.051341        0.079453   \n",
       "20                          0.035773    0.045514        0.072627   \n",
       "21                          0.043831    0.051814        0.079821   \n",
       "22                          0.059161    0.079257        0.115558   \n",
       "23                          0.041058    0.048332        0.076475   \n",
       "24                          0.047646    0.055806        0.086082   \n",
       "25                          0.038107    0.049051        0.076851   \n",
       "26                          0.039114    0.047648        0.075827   \n",
       "27                          0.042050    0.048635        0.075757   \n",
       "28                          0.035541    0.045778        0.074046   \n",
       "29                          0.046682    0.055123        0.086023   \n",
       "...                              ...         ...             ...   \n",
       "10327                       0.046165    0.055873        0.086208   \n",
       "10328                       0.047890    0.056737        0.088046   \n",
       "10329                       0.042718    0.049121        0.077197   \n",
       "10330                       0.048269    0.057532        0.087245   \n",
       "10331                       0.037413    0.046543        0.075702   \n",
       "10332                       0.046018    0.053857        0.082575   \n",
       "10333                       0.043493    0.050290        0.078750   \n",
       "10334                       0.048579    0.060856        0.095357   \n",
       "10335                       0.046129    0.055451        0.085807   \n",
       "10336                       0.040467    0.052110        0.082008   \n",
       "10337                       0.044774    0.052164        0.080056   \n",
       "10338                       0.042613    0.049485        0.077390   \n",
       "10339                       0.043509    0.049774        0.077161   \n",
       "10340                       0.043389    0.053241        0.083601   \n",
       "10341                       0.038922    0.048638        0.081346   \n",
       "10342                       0.054619    0.066664        0.100851   \n",
       "10343                       0.043619    0.051481        0.082140   \n",
       "10344                       0.042381    0.048728        0.076103   \n",
       "10345                       0.038739    0.047075        0.075612   \n",
       "10346                       0.043318    0.049435        0.077688   \n",
       "10347                       0.042408    0.048680        0.077097   \n",
       "10348                       0.046610    0.054734        0.084658   \n",
       "10349                       0.042575    0.049427        0.078086   \n",
       "10350                       0.042930    0.050663        0.078106   \n",
       "10351                       0.041143    0.049928        0.076575   \n",
       "10352                       0.041385    0.047826        0.075369   \n",
       "10353                       0.043502    0.049593        0.077350   \n",
       "10354                       0.040569    0.047341        0.075667   \n",
       "10355                       0.060326    0.088018        0.128947   \n",
       "10356                       0.044267    0.052154        0.080761   \n",
       "\n",
       "       Plasma membrane  Cell junctions  Mitochondria  Aggresome   Cytosol  \\\n",
       "0             0.077283        0.047587      0.201696   0.029957  0.294167   \n",
       "1             0.087392        0.058937      0.214846   0.038274  0.310294   \n",
       "2             0.072260        0.050324      0.181006   0.033442  0.247697   \n",
       "3             0.073665        0.052880      0.180115   0.036528  0.237169   \n",
       "4             0.108494        0.070740      0.226998   0.048502  0.313710   \n",
       "5             0.084409        0.056152      0.192301   0.037529  0.269018   \n",
       "6             0.075983        0.053265      0.185011   0.036509  0.250154   \n",
       "7             0.083146        0.054772      0.194876   0.035535  0.277231   \n",
       "8             0.081620        0.057115      0.193401   0.039230  0.261425   \n",
       "9             0.108332        0.068749      0.228085   0.048842  0.320969   \n",
       "10            0.085105        0.059329      0.198728   0.040632  0.273289   \n",
       "11            0.075644        0.052886      0.182380   0.035849  0.249338   \n",
       "12            0.071750        0.050548      0.178848   0.034097  0.239425   \n",
       "13            0.090954        0.057350      0.204092   0.037261  0.301040   \n",
       "14            0.071963        0.051094      0.178207   0.035082  0.236767   \n",
       "15            0.114618        0.075269      0.241362   0.051826  0.343999   \n",
       "16            0.089717        0.055552      0.198189   0.036859  0.301756   \n",
       "17            0.075404        0.054415      0.185788   0.037152  0.245107   \n",
       "18            0.098839        0.072290      0.209661   0.049867  0.277919   \n",
       "19            0.076575        0.054467      0.185180   0.037301  0.243903   \n",
       "20            0.078245        0.049752      0.189759   0.031933  0.271735   \n",
       "21            0.078180        0.055190      0.187673   0.037880  0.251378   \n",
       "22            0.129168        0.085231      0.250504   0.056858  0.353541   \n",
       "23            0.073970        0.051329      0.182947   0.034972  0.246795   \n",
       "24            0.085365        0.059598      0.198664   0.040885  0.266808   \n",
       "25            0.079816        0.051605      0.195505   0.033888  0.277962   \n",
       "26            0.077002        0.050681      0.187232   0.034241  0.257908   \n",
       "27            0.072704        0.051415      0.178526   0.035460  0.237836   \n",
       "28            0.077531        0.049376      0.189814   0.031392  0.280886   \n",
       "29            0.085686        0.058992      0.197959   0.040591  0.266147   \n",
       "...                ...             ...           ...        ...       ...   \n",
       "10327         0.087094        0.062228      0.199037   0.040919  0.275932   \n",
       "10328         0.088980        0.059789      0.202542   0.041517  0.268485   \n",
       "10329         0.075419        0.053155      0.182308   0.036727  0.244835   \n",
       "10330         0.086143        0.060373      0.199136   0.041135  0.264660   \n",
       "10331         0.077737        0.048355      0.193705   0.032067  0.280083   \n",
       "10332         0.080278        0.055444      0.190571   0.038635  0.255272   \n",
       "10333         0.076683        0.052314      0.184907   0.037342  0.248744   \n",
       "10334         0.098504        0.066875      0.213461   0.045156  0.299545   \n",
       "10335         0.086317        0.058653      0.196654   0.040518  0.269475   \n",
       "10336         0.084750        0.054178      0.199879   0.037066  0.290542   \n",
       "10337         0.077359        0.055083      0.185534   0.038071  0.246365   \n",
       "10338         0.074884        0.052248      0.183005   0.035816  0.245316   \n",
       "10339         0.073894        0.052820      0.179420   0.036539  0.236862   \n",
       "10340         0.083023        0.054466      0.198563   0.037296  0.277539   \n",
       "10341         0.083294        0.053679      0.199001   0.036478  0.285791   \n",
       "10342         0.104150        0.070698      0.222030   0.049413  0.304199   \n",
       "10343         0.083116        0.056430      0.196752   0.037587  0.268611   \n",
       "10344         0.072734        0.051609      0.178745   0.035682  0.237223   \n",
       "10345         0.074661        0.050013      0.183614   0.033689  0.257364   \n",
       "10346         0.074299        0.052748      0.181529   0.036464  0.241181   \n",
       "10347         0.073864        0.052056      0.181388   0.035822  0.241384   \n",
       "10348         0.082378        0.057442      0.192978   0.040486  0.258071   \n",
       "10349         0.074892        0.052721      0.183757   0.036327  0.245253   \n",
       "10350         0.076249        0.053032      0.185045   0.035841  0.248719   \n",
       "10351         0.076969        0.051462      0.184648   0.035596  0.253380   \n",
       "10352         0.072273        0.051173      0.179841   0.034851  0.242434   \n",
       "10353         0.073420        0.052536      0.179119   0.036418  0.236215   \n",
       "10354         0.072696        0.050657      0.180077   0.034638  0.242694   \n",
       "10355         0.154988        0.094236      0.274464   0.064098  0.414894   \n",
       "10356         0.080050        0.054657      0.187942   0.037499  0.252940   \n",
       "\n",
       "       Cytoplasmic bodies  Rods & rings  \n",
       "0                0.037141      0.030636  \n",
       "1                0.045202      0.039707  \n",
       "2                0.037618      0.032566  \n",
       "3                0.039492      0.034519  \n",
       "4                0.058137      0.049274  \n",
       "5                0.044169      0.037816  \n",
       "6                0.040464      0.035083  \n",
       "7                0.042854      0.036196  \n",
       "8                0.044124      0.037734  \n",
       "9                0.059272      0.049704  \n",
       "10               0.046041      0.040255  \n",
       "11               0.039789      0.034190  \n",
       "12               0.037326      0.032480  \n",
       "13               0.046354      0.036565  \n",
       "14               0.038306      0.033256  \n",
       "15               0.062043      0.051574  \n",
       "16               0.045788      0.039982  \n",
       "17               0.040822      0.035423  \n",
       "18               0.056545      0.049223  \n",
       "19               0.040851      0.035646  \n",
       "20               0.038275      0.033056  \n",
       "21               0.041526      0.036387  \n",
       "22               0.072414      0.060922  \n",
       "23               0.038797      0.033528  \n",
       "24               0.046630      0.040404  \n",
       "25               0.040162      0.033968  \n",
       "26               0.038382      0.033737  \n",
       "27               0.038708      0.033685  \n",
       "28               0.038653      0.032006  \n",
       "29               0.046159      0.039713  \n",
       "...                   ...           ...  \n",
       "10327            0.046745      0.040172  \n",
       "10328            0.047557      0.041321  \n",
       "10329            0.040456      0.035270  \n",
       "10330            0.046324      0.040735  \n",
       "10331            0.039765      0.032234  \n",
       "10332            0.043669      0.037375  \n",
       "10333            0.041084      0.034968  \n",
       "10334            0.054155      0.045559  \n",
       "10335            0.045936      0.039698  \n",
       "10336            0.043887      0.037356  \n",
       "10337            0.041690      0.036610  \n",
       "10338            0.039532      0.034953  \n",
       "10339            0.039634      0.034688  \n",
       "10340            0.046303      0.037051  \n",
       "10341            0.042826      0.034850  \n",
       "10342            0.059816      0.048824  \n",
       "10343            0.044510      0.037251  \n",
       "10344            0.038715      0.033782  \n",
       "10345            0.038625      0.032746  \n",
       "10346            0.039799      0.034712  \n",
       "10347            0.039455      0.033925  \n",
       "10348            0.044954      0.038869  \n",
       "10349            0.039925      0.034570  \n",
       "10350            0.041125      0.034899  \n",
       "10351            0.039656      0.034866  \n",
       "10352            0.038577      0.033225  \n",
       "10353            0.039359      0.034318  \n",
       "10354            0.038069      0.032842  \n",
       "10355            0.087967      0.070674  \n",
       "10356            0.042884      0.036133  \n",
       "\n",
       "[10357 rows x 28 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_proba_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12509767357886614562\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11273211085\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17876244925859581285\n",
      "physical_device_desc: \"device: 0, name: Tesla K40m, pci bus id: 0000:04:00.0, compute capability: 3.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def base_f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return f1\n",
    "\n",
    "def f1_min(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.min(f1)\n",
    "\n",
    "def f1_max(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.max(f1)\n",
    "\n",
    "def f1_mean(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_std(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.std(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrackHistory(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImprovedDataGenerator(DataGenerator):\n",
    "    \n",
    "    # in contrast to the base DataGenerator we add a target wishlist to init\n",
    "    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor, target_wishlist):\n",
    "        super().__init__(list_IDs, labels, modelparameter, imagepreprocessor)\n",
    "        self.target_wishlist = target_wishlist\n",
    "    \n",
    "    def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier][self.target_wishlist].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_dropout = False\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "class ImprovedModel(BaseLineModel):\n",
    "    \n",
    "    def __init__(self, modelparameter,\n",
    "                 use_dropout,\n",
    "                 my_metrics=[f1_mean, f1_std, f1_min, f1_max]):\n",
    "        \n",
    "        super().__init__(modelparameter)\n",
    "        self.my_metrics = my_metrics\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def learn(self):\n",
    "        self.history = TrackHistory()\n",
    "        return self.model.fit_generator(generator=self.training_generator,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    epochs=self.params.n_epochs, \n",
    "                    use_multiprocessing=True,\n",
    "                    workers=8,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_steps=50,                                     \n",
    "                    callbacks = [self.history])\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "\n",
    "        # Block 1\n",
    "        self.model.add(Conv2D(64, (3, 3), padding='same', name='conv1_1', input_shape = self.input_shape))\n",
    "        self.model.add(BatchNormalization(name='bn1_1'))\n",
    "        self.model.add(Activation('relu', name='relu1_1'))\n",
    "        self.model.add(Conv2D(64, (3, 3), padding='same', name='conv1_2'))\n",
    "        self.model.add(BatchNormalization(name='bn1_2'))\n",
    "        self.model.add(Activation('relu', name='relu1_2'))\n",
    "        self.model.add(MaxPooling2D((2, 2), strides=(2, 2), name='pool1'))\n",
    "\n",
    "        # Block 2\n",
    "        self.model.add(Conv2D(128, (3, 3), padding='same', name='conv2_1'))\n",
    "        self.model.add(BatchNormalization(name='bn2_1'))\n",
    "        self.model.add(Activation('relu', name='relu2_1'))\n",
    "        self.model.add(Conv2D(128, (3, 3), padding='same', name='conv2_2'))\n",
    "        self.model.add(BatchNormalization(name='bn2_2'))\n",
    "        self.model.add(Activation('relu', name='relu2_2'))\n",
    "        self.model.add(MaxPooling2D((2, 2), strides=(2, 2), name='pool2'))\n",
    "\n",
    "        # Block 3\n",
    "        self.model.add(Conv2D(256, (3, 3), padding='same', name='conv3_1'))\n",
    "        self.model.add(BatchNormalization(name='bn3_1'))\n",
    "        self.model.add(Activation('relu', name='relu3_1'))\n",
    "        self.model.add(Conv2D(256, (3, 3), padding='same', name='conv3_2'))\n",
    "        self.model.add(BatchNormalization(name='bn3_2'))\n",
    "        self.model.add(Activation('relu', name='relu3_2'))\n",
    "        self.model.add(Conv2D(256, (3, 3), padding='same', name='conv3_3'))\n",
    "        self.model.add(BatchNormalization(name='bn3_3'))\n",
    "        self.model.add(Activation('relu', name='relu3_3'))\n",
    "        self.model.add(Conv2D(256, (3, 3), padding='same', name='conv3_4'))\n",
    "        self.model.add(BatchNormalization(name='bn3_4'))\n",
    "        self.model.add(Activation('relu', name='relu3_4'))\n",
    "        self.model.add(MaxPooling2D((2, 2), strides=(2, 2), name='pool3'))\n",
    "\n",
    "        # Classification block\n",
    "        self.model.add(Flatten(name='flatten'))\n",
    "        self.model.add(Dense(512, name='ip1'))\n",
    "        self.model.add(BatchNormalization(name='bn4'))\n",
    "        self.model.add(Activation('relu', name='relu4'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(512, name='ip2'))\n",
    "        self.model.add(BatchNormalization(name='bn5'))\n",
    "        self.model.add(Activation('relu', name='relu5'))\n",
    "        self.model.add(Dense(28, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter = ModelParameters()\n",
    "preprocessor = ImagePreprocessor(parameter)\n",
    "labels = train_labels\n",
    "training_generator = DataGenerator(partition['train'], labels,\n",
    "                                           parameter, preprocessor)\n",
    "validation_generator = DataGenerator(partition['validation'], labels,\n",
    "                                             parameter, preprocessor)\n",
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "100/100 [==============================] - 87s - loss: 0.1902 - f1_mean: 0.0118 - f1_std: 0.0490 - f1_min: 0.0000e+00 - f1_max: 0.2416 - val_loss: 0.1853 - val_f1_mean: 0.0000e+00 - val_f1_std: 0.0000e+00 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0000e+00\n",
      "Epoch 2/300\n",
      "100/100 [==============================] - 68s - loss: 0.1769 - f1_mean: 0.0143 - f1_std: 0.0639 - f1_min: 0.0000e+00 - f1_max: 0.3248 - val_loss: 0.1852 - val_f1_mean: 0.0000e+00 - val_f1_std: 0.0000e+00 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0000e+00\n",
      "Epoch 3/300\n",
      "100/100 [==============================] - 67s - loss: 0.1721 - f1_mean: 0.0145 - f1_std: 0.0650 - f1_min: 0.0000e+00 - f1_max: 0.3323 - val_loss: 0.1815 - val_f1_mean: 7.0683e-04 - val_f1_std: 0.0037 - val_f1_min: 0.0000e+00 - val_f1_max: 0.0198\n",
      "Epoch 4/300\n",
      "100/100 [==============================] - 66s - loss: 0.1701 - f1_mean: 0.0150 - f1_std: 0.0677 - f1_min: 0.0000e+00 - f1_max: 0.3463 - val_loss: 0.1799 - val_f1_mean: 0.0106 - val_f1_std: 0.0552 - val_f1_min: 0.0000e+00 - val_f1_max: 0.2975\n",
      "Epoch 5/300\n",
      "100/100 [==============================] - 69s - loss: 0.1648 - f1_mean: 0.0162 - f1_std: 0.0727 - f1_min: 0.0000e+00 - f1_max: 0.3730 - val_loss: 0.1654 - val_f1_mean: 0.0123 - val_f1_std: 0.0623 - val_f1_min: 0.0000e+00 - val_f1_max: 0.3334\n",
      "Epoch 6/300\n",
      "100/100 [==============================] - 68s - loss: 0.1635 - f1_mean: 0.0199 - f1_std: 0.0817 - f1_min: 0.0000e+00 - f1_max: 0.4030 - val_loss: 0.1670 - val_f1_mean: 0.0071 - val_f1_std: 0.0355 - val_f1_min: 0.0000e+00 - val_f1_max: 0.1896\n",
      "Epoch 7/300\n",
      "100/100 [==============================] - 68s - loss: 0.1634 - f1_mean: 0.0258 - f1_std: 0.0993 - f1_min: 0.0000e+00 - f1_max: 0.4757 - val_loss: 0.1724 - val_f1_mean: 0.0208 - val_f1_std: 0.0865 - val_f1_min: 0.0000e+00 - val_f1_max: 0.4268\n",
      "Epoch 8/300\n",
      "100/100 [==============================] - 68s - loss: 0.1593 - f1_mean: 0.0342 - f1_std: 0.1180 - f1_min: 0.0000e+00 - f1_max: 0.5358 - val_loss: 0.1957 - val_f1_mean: 0.0171 - val_f1_std: 0.0807 - val_f1_min: 0.0000e+00 - val_f1_max: 0.4241\n",
      "Epoch 9/300\n",
      "100/100 [==============================] - 67s - loss: 0.1564 - f1_mean: 0.0422 - f1_std: 0.1332 - f1_min: 0.0000e+00 - f1_max: 0.5824 - val_loss: 0.1629 - val_f1_mean: 0.0298 - val_f1_std: 0.1057 - val_f1_min: 0.0000e+00 - val_f1_max: 0.4800\n",
      "Epoch 10/300\n",
      "100/100 [==============================] - 67s - loss: 0.1503 - f1_mean: 0.0512 - f1_std: 0.1514 - f1_min: 0.0000e+00 - f1_max: 0.6415 - val_loss: 0.2049 - val_f1_mean: 0.0210 - val_f1_std: 0.0829 - val_f1_min: 0.0000e+00 - val_f1_max: 0.3982\n",
      "Epoch 11/300\n",
      "100/100 [==============================] - 70s - loss: 0.1530 - f1_mean: 0.0526 - f1_std: 0.1547 - f1_min: 0.0000e+00 - f1_max: 0.6649 - val_loss: 0.1659 - val_f1_mean: 0.0409 - val_f1_std: 0.1532 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7116\n",
      "Epoch 12/300\n",
      "100/100 [==============================] - 68s - loss: 0.1509 - f1_mean: 0.0525 - f1_std: 0.1525 - f1_min: 0.0000e+00 - f1_max: 0.6375 - val_loss: 0.1673 - val_f1_mean: 0.0401 - val_f1_std: 0.1346 - val_f1_min: 0.0000e+00 - val_f1_max: 0.6176\n",
      "Epoch 13/300\n",
      "100/100 [==============================] - 67s - loss: 0.1437 - f1_mean: 0.0623 - f1_std: 0.1679 - f1_min: 0.0000e+00 - f1_max: 0.6837 - val_loss: 0.1640 - val_f1_mean: 0.0292 - val_f1_std: 0.1027 - val_f1_min: 0.0000e+00 - val_f1_max: 0.4873\n",
      "Epoch 14/300\n",
      "100/100 [==============================] - 67s - loss: 0.1440 - f1_mean: 0.0659 - f1_std: 0.1731 - f1_min: 0.0000e+00 - f1_max: 0.7085 - val_loss: 0.1606 - val_f1_mean: 0.0494 - val_f1_std: 0.1456 - val_f1_min: 0.0000e+00 - val_f1_max: 0.6259\n",
      "Epoch 15/300\n",
      "100/100 [==============================] - 68s - loss: 0.1431 - f1_mean: 0.0745 - f1_std: 0.1853 - f1_min: 0.0000e+00 - f1_max: 0.7292 - val_loss: 0.1713 - val_f1_mean: 0.0365 - val_f1_std: 0.1275 - val_f1_min: 0.0000e+00 - val_f1_max: 0.6135\n",
      "Epoch 16/300\n",
      "100/100 [==============================] - 67s - loss: 0.1443 - f1_mean: 0.0727 - f1_std: 0.1816 - f1_min: 0.0000e+00 - f1_max: 0.7293 - val_loss: 0.1651 - val_f1_mean: 0.0553 - val_f1_std: 0.1648 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7237\n",
      "Epoch 17/300\n",
      "100/100 [==============================] - 70s - loss: 0.1395 - f1_mean: 0.0835 - f1_std: 0.1995 - f1_min: 0.0000e+00 - f1_max: 0.7749 - val_loss: 0.1587 - val_f1_mean: 0.0484 - val_f1_std: 0.1349 - val_f1_min: 0.0000e+00 - val_f1_max: 0.5831\n",
      "Epoch 18/300\n",
      "100/100 [==============================] - 67s - loss: 0.1327 - f1_mean: 0.0995 - f1_std: 0.2163 - f1_min: 0.0000e+00 - f1_max: 0.8117 - val_loss: 0.1668 - val_f1_mean: 0.0555 - val_f1_std: 0.1703 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7159\n",
      "Epoch 19/300\n",
      "100/100 [==============================] - 67s - loss: 0.1334 - f1_mean: 0.0997 - f1_std: 0.2142 - f1_min: 0.0000e+00 - f1_max: 0.7936 - val_loss: 0.1679 - val_f1_mean: 0.0632 - val_f1_std: 0.1676 - val_f1_min: 0.0000e+00 - val_f1_max: 0.6937\n",
      "Epoch 20/300\n",
      "100/100 [==============================] - 66s - loss: 0.1325 - f1_mean: 0.1056 - f1_std: 0.2189 - f1_min: 0.0000e+00 - f1_max: 0.7952 - val_loss: 0.1492 - val_f1_mean: 0.0687 - val_f1_std: 0.1661 - val_f1_min: 0.0000e+00 - val_f1_max: 0.6486\n",
      "Epoch 21/300\n",
      "100/100 [==============================] - 67s - loss: 0.1326 - f1_mean: 0.1079 - f1_std: 0.2239 - f1_min: 0.0000e+00 - f1_max: 0.8143 - val_loss: 0.1642 - val_f1_mean: 0.0531 - val_f1_std: 0.1745 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7695\n",
      "Epoch 22/300\n",
      "100/100 [==============================] - 68s - loss: 0.1226 - f1_mean: 0.1323 - f1_std: 0.2458 - f1_min: 0.0000e+00 - f1_max: 0.8636 - val_loss: 0.1591 - val_f1_mean: 0.0668 - val_f1_std: 0.1749 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7009\n",
      "Epoch 23/300\n",
      "100/100 [==============================] - 70s - loss: 0.1226 - f1_mean: 0.1355 - f1_std: 0.2532 - f1_min: 0.0000e+00 - f1_max: 0.8707 - val_loss: 0.1558 - val_f1_mean: 0.0738 - val_f1_std: 0.1829 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7022\n",
      "Epoch 24/300\n",
      "100/100 [==============================] - 68s - loss: 0.1230 - f1_mean: 0.1292 - f1_std: 0.2401 - f1_min: 0.0000e+00 - f1_max: 0.8231 - val_loss: 0.1583 - val_f1_mean: 0.0714 - val_f1_std: 0.1903 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7693\n",
      "Epoch 25/300\n",
      "100/100 [==============================] - 68s - loss: 0.1194 - f1_mean: 0.1513 - f1_std: 0.2666 - f1_min: 0.0000e+00 - f1_max: 0.8919 - val_loss: 0.1503 - val_f1_mean: 0.0912 - val_f1_std: 0.1975 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7170\n",
      "Epoch 26/300\n",
      "100/100 [==============================] - 68s - loss: 0.1074 - f1_mean: 0.1794 - f1_std: 0.2884 - f1_min: 0.0000e+00 - f1_max: 0.9269 - val_loss: 0.1637 - val_f1_mean: 0.0687 - val_f1_std: 0.1890 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7669\n",
      "Epoch 27/300\n",
      "100/100 [==============================] - 68s - loss: 0.1137 - f1_mean: 0.1771 - f1_std: 0.2842 - f1_min: 0.0000e+00 - f1_max: 0.9227 - val_loss: 0.1682 - val_f1_mean: 0.0758 - val_f1_std: 0.1841 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7415\n",
      "Epoch 28/300\n",
      "100/100 [==============================] - 68s - loss: 0.1096 - f1_mean: 0.1859 - f1_std: 0.2901 - f1_min: 0.0000e+00 - f1_max: 0.9143 - val_loss: 0.1613 - val_f1_mean: 0.0901 - val_f1_std: 0.2023 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7878\n",
      "Epoch 29/300\n",
      "100/100 [==============================] - 71s - loss: 0.1109 - f1_mean: 0.1808 - f1_std: 0.2928 - f1_min: 0.0000e+00 - f1_max: 0.9318 - val_loss: 0.1610 - val_f1_mean: 0.0800 - val_f1_std: 0.1885 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7196\n",
      "Epoch 30/300\n",
      "100/100 [==============================] - 68s - loss: 0.0945 - f1_mean: 0.2315 - f1_std: 0.3208 - f1_min: 0.0000e+00 - f1_max: 0.9636 - val_loss: 0.1980 - val_f1_mean: 0.0805 - val_f1_std: 0.1988 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7981\n",
      "Epoch 31/300\n",
      "100/100 [==============================] - 69s - loss: 0.0944 - f1_mean: 0.2363 - f1_std: 0.3267 - f1_min: 0.0000e+00 - f1_max: 0.9492 - val_loss: 0.1647 - val_f1_mean: 0.1001 - val_f1_std: 0.2156 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7626\n",
      "Epoch 32/300\n",
      "100/100 [==============================] - 69s - loss: 0.0986 - f1_mean: 0.2267 - f1_std: 0.3213 - f1_min: 0.0000e+00 - f1_max: 0.9719 - val_loss: 0.1565 - val_f1_mean: 0.1018 - val_f1_std: 0.2204 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7873\n",
      "Epoch 33/300\n",
      "100/100 [==============================] - 69s - loss: 0.0988 - f1_mean: 0.2330 - f1_std: 0.3245 - f1_min: 0.0000e+00 - f1_max: 0.9600 - val_loss: 0.1659 - val_f1_mean: 0.0835 - val_f1_std: 0.1962 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7664\n",
      "Epoch 34/300\n",
      "100/100 [==============================] - 70s - loss: 0.0829 - f1_mean: 0.2834 - f1_std: 0.3531 - f1_min: 0.0000e+00 - f1_max: 0.9783 - val_loss: 0.1824 - val_f1_mean: 0.0757 - val_f1_std: 0.1796 - val_f1_min: 0.0000e+00 - val_f1_max: 0.6792\n",
      "Epoch 35/300\n",
      "100/100 [==============================] - 72s - loss: 0.0798 - f1_mean: 0.2985 - f1_std: 0.3625 - f1_min: 0.0000e+00 - f1_max: 0.9966 - val_loss: 0.1719 - val_f1_mean: 0.0840 - val_f1_std: 0.1965 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7388\n",
      "Epoch 36/300\n",
      "100/100 [==============================] - 68s - loss: 0.0858 - f1_mean: 0.2922 - f1_std: 0.3549 - f1_min: 0.0000e+00 - f1_max: 0.9890 - val_loss: 0.1661 - val_f1_mean: 0.0982 - val_f1_std: 0.2134 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7722\n",
      "Epoch 37/300\n",
      "100/100 [==============================] - 68s - loss: 0.0868 - f1_mean: 0.2801 - f1_std: 0.3517 - f1_min: 0.0000e+00 - f1_max: 0.9790 - val_loss: 0.1690 - val_f1_mean: 0.1295 - val_f1_std: 0.2407 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8512\n",
      "Epoch 38/300\n",
      "100/100 [==============================] - 67s - loss: 0.0752 - f1_mean: 0.3403 - f1_std: 0.3819 - f1_min: 0.0000e+00 - f1_max: 0.9980 - val_loss: 0.1858 - val_f1_mean: 0.0866 - val_f1_std: 0.1932 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7226\n",
      "Epoch 39/300\n",
      "100/100 [==============================] - 67s - loss: 0.0702 - f1_mean: 0.3458 - f1_std: 0.3844 - f1_min: 0.0000e+00 - f1_max: 0.9991 - val_loss: 0.1713 - val_f1_mean: 0.1094 - val_f1_std: 0.2259 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7976\n",
      "Epoch 40/300\n",
      "100/100 [==============================] - 68s - loss: 0.0737 - f1_mean: 0.3334 - f1_std: 0.3782 - f1_min: 0.0000e+00 - f1_max: 0.9958 - val_loss: 0.1738 - val_f1_mean: 0.0979 - val_f1_std: 0.2048 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7597\n",
      "Epoch 41/300\n",
      "100/100 [==============================] - 71s - loss: 0.0747 - f1_mean: 0.3355 - f1_std: 0.3793 - f1_min: 0.0000e+00 - f1_max: 0.9975 - val_loss: 0.1753 - val_f1_mean: 0.1229 - val_f1_std: 0.2288 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7983\n",
      "Epoch 42/300\n",
      "100/100 [==============================] - 69s - loss: 0.0707 - f1_mean: 0.3640 - f1_std: 0.3878 - f1_min: 0.0000e+00 - f1_max: 0.9935 - val_loss: 0.1751 - val_f1_mean: 0.1004 - val_f1_std: 0.2242 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8279\n",
      "Epoch 43/300\n",
      "100/100 [==============================] - 68s - loss: 0.0646 - f1_mean: 0.3819 - f1_std: 0.4002 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1695 - val_f1_mean: 0.1140 - val_f1_std: 0.2225 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7785\n",
      "Epoch 44/300\n",
      "100/100 [==============================] - 69s - loss: 0.0652 - f1_mean: 0.3751 - f1_std: 0.3975 - f1_min: 0.0000e+00 - f1_max: 0.9989 - val_loss: 0.1733 - val_f1_mean: 0.1152 - val_f1_std: 0.2180 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7490\n",
      "Epoch 45/300\n",
      "100/100 [==============================] - 68s - loss: 0.0678 - f1_mean: 0.3739 - f1_std: 0.3962 - f1_min: 0.0000e+00 - f1_max: 0.9971 - val_loss: 0.1878 - val_f1_mean: 0.1279 - val_f1_std: 0.2396 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8829\n",
      "Epoch 46/300\n",
      "100/100 [==============================] - 68s - loss: 0.0656 - f1_mean: 0.3808 - f1_std: 0.3986 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1576 - val_f1_mean: 0.1344 - val_f1_std: 0.2445 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8070\n",
      "Epoch 47/300\n",
      "100/100 [==============================] - 70s - loss: 0.0619 - f1_mean: 0.3950 - f1_std: 0.4052 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1991 - val_f1_mean: 0.0972 - val_f1_std: 0.1918 - val_f1_min: 0.0000e+00 - val_f1_max: 0.6799\n",
      "Epoch 48/300\n",
      "100/100 [==============================] - 69s - loss: 0.0622 - f1_mean: 0.3925 - f1_std: 0.4031 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.2003 - val_f1_mean: 0.0870 - val_f1_std: 0.1846 - val_f1_min: 0.0000e+00 - val_f1_max: 0.6494\n",
      "Epoch 49/300\n",
      "100/100 [==============================] - 67s - loss: 0.0633 - f1_mean: 0.3923 - f1_std: 0.4017 - f1_min: 0.0000e+00 - f1_max: 0.9983 - val_loss: 0.1761 - val_f1_mean: 0.1137 - val_f1_std: 0.2209 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7791\n",
      "Epoch 50/300\n",
      "100/100 [==============================] - 68s - loss: 0.0605 - f1_mean: 0.4028 - f1_std: 0.4060 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1887 - val_f1_mean: 0.1103 - val_f1_std: 0.2311 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8304\n",
      "Epoch 51/300\n",
      "100/100 [==============================] - 69s - loss: 0.0574 - f1_mean: 0.4108 - f1_std: 0.4129 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1695 - val_f1_mean: 0.1412 - val_f1_std: 0.2523 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8369\n",
      "Epoch 52/300\n",
      "100/100 [==============================] - 67s - loss: 0.0592 - f1_mean: 0.4094 - f1_std: 0.4082 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1832 - val_f1_mean: 0.1281 - val_f1_std: 0.2423 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8304\n",
      "Epoch 53/300\n",
      "100/100 [==============================] - 71s - loss: 0.0595 - f1_mean: 0.4046 - f1_std: 0.4084 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1632 - val_f1_mean: 0.1411 - val_f1_std: 0.2493 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8404\n",
      "Epoch 54/300\n",
      "100/100 [==============================] - 69s - loss: 0.0596 - f1_mean: 0.4071 - f1_std: 0.4075 - f1_min: 0.0000e+00 - f1_max: 0.9986 - val_loss: 0.1720 - val_f1_mean: 0.1436 - val_f1_std: 0.2534 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8798\n",
      "Epoch 55/300\n",
      "100/100 [==============================] - 68s - loss: 0.0561 - f1_mean: 0.4267 - f1_std: 0.4157 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.2020 - val_f1_mean: 0.0978 - val_f1_std: 0.2001 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7006\n",
      "Epoch 56/300\n",
      "100/100 [==============================] - 68s - loss: 0.0573 - f1_mean: 0.4169 - f1_std: 0.4125 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1692 - val_f1_mean: 0.1358 - val_f1_std: 0.2447 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8311\n",
      "Epoch 57/300\n",
      "100/100 [==============================] - 69s - loss: 0.0569 - f1_mean: 0.4118 - f1_std: 0.4124 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1690 - val_f1_mean: 0.1246 - val_f1_std: 0.2399 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8303\n",
      "Epoch 58/300\n",
      "100/100 [==============================] - 67s - loss: 0.0571 - f1_mean: 0.4128 - f1_std: 0.4097 - f1_min: 0.0000e+00 - f1_max: 0.9986 - val_loss: 0.1694 - val_f1_mean: 0.1275 - val_f1_std: 0.2412 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8417\n",
      "Epoch 59/300\n",
      "100/100 [==============================] - 70s - loss: 0.0532 - f1_mean: 0.4373 - f1_std: 0.4190 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1827 - val_f1_mean: 0.1240 - val_f1_std: 0.2333 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7873\n",
      "Epoch 60/300\n",
      "100/100 [==============================] - 68s - loss: 0.0551 - f1_mean: 0.4171 - f1_std: 0.4100 - f1_min: 0.0000e+00 - f1_max: 0.9989 - val_loss: 0.1710 - val_f1_mean: 0.1363 - val_f1_std: 0.2487 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8608\n",
      "Epoch 61/300\n",
      "100/100 [==============================] - 68s - loss: 0.0550 - f1_mean: 0.4184 - f1_std: 0.4116 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1808 - val_f1_mean: 0.1129 - val_f1_std: 0.2269 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8113\n",
      "Epoch 62/300\n",
      "100/100 [==============================] - 69s - loss: 0.0549 - f1_mean: 0.4227 - f1_std: 0.4134 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1727 - val_f1_mean: 0.1296 - val_f1_std: 0.2322 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7939\n",
      "Epoch 63/300\n",
      "100/100 [==============================] - 68s - loss: 0.0534 - f1_mean: 0.4188 - f1_std: 0.4161 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1825 - val_f1_mean: 0.1332 - val_f1_std: 0.2413 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8211\n",
      "Epoch 64/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 68s - loss: 0.0524 - f1_mean: 0.4255 - f1_std: 0.4185 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1621 - val_f1_mean: 0.1539 - val_f1_std: 0.2656 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8728\n",
      "Epoch 65/300\n",
      "100/100 [==============================] - 70s - loss: 0.0525 - f1_mean: 0.4321 - f1_std: 0.4168 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1966 - val_f1_mean: 0.1173 - val_f1_std: 0.2247 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7642\n",
      "Epoch 66/300\n",
      "100/100 [==============================] - 68s - loss: 0.0528 - f1_mean: 0.4300 - f1_std: 0.4143 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1807 - val_f1_mean: 0.1214 - val_f1_std: 0.2282 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7730\n",
      "Epoch 67/300\n",
      "100/100 [==============================] - 69s - loss: 0.0532 - f1_mean: 0.4353 - f1_std: 0.4176 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1691 - val_f1_mean: 0.1226 - val_f1_std: 0.2310 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8094\n",
      "Epoch 68/300\n",
      "100/100 [==============================] - 68s - loss: 0.0506 - f1_mean: 0.4176 - f1_std: 0.4161 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1560 - val_f1_mean: 0.1341 - val_f1_std: 0.2417 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8228\n",
      "Epoch 69/300\n",
      "100/100 [==============================] - 68s - loss: 0.0512 - f1_mean: 0.4214 - f1_std: 0.4152 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1692 - val_f1_mean: 0.1131 - val_f1_std: 0.2211 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7450\n",
      "Epoch 70/300\n",
      "100/100 [==============================] - 68s - loss: 0.0522 - f1_mean: 0.4302 - f1_std: 0.4157 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1648 - val_f1_mean: 0.1462 - val_f1_std: 0.2503 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8485\n",
      "Epoch 71/300\n",
      "100/100 [==============================] - 70s - loss: 0.0513 - f1_mean: 0.4395 - f1_std: 0.4181 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1650 - val_f1_mean: 0.1143 - val_f1_std: 0.2222 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8027\n",
      "Epoch 72/300\n",
      "100/100 [==============================] - 68s - loss: 0.0503 - f1_mean: 0.4324 - f1_std: 0.4181 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1649 - val_f1_mean: 0.1420 - val_f1_std: 0.2592 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8727\n",
      "Epoch 73/300\n",
      "100/100 [==============================] - 68s - loss: 0.0493 - f1_mean: 0.4297 - f1_std: 0.4173 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1763 - val_f1_mean: 0.1307 - val_f1_std: 0.2444 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8382\n",
      "Epoch 74/300\n",
      "100/100 [==============================] - 69s - loss: 0.0509 - f1_mean: 0.4309 - f1_std: 0.4176 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1567 - val_f1_mean: 0.1341 - val_f1_std: 0.2460 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8405\n",
      "Epoch 75/300\n",
      "100/100 [==============================] - 68s - loss: 0.0503 - f1_mean: 0.4341 - f1_std: 0.4179 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1748 - val_f1_mean: 0.1427 - val_f1_std: 0.2534 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8470\n",
      "Epoch 76/300\n",
      "100/100 [==============================] - 69s - loss: 0.0506 - f1_mean: 0.4258 - f1_std: 0.4152 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1674 - val_f1_mean: 0.1527 - val_f1_std: 0.2540 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8434\n",
      "Epoch 77/300\n",
      "100/100 [==============================] - 71s - loss: 0.0473 - f1_mean: 0.4341 - f1_std: 0.4197 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1698 - val_f1_mean: 0.1450 - val_f1_std: 0.2557 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8415\n",
      "Epoch 78/300\n",
      "100/100 [==============================] - 68s - loss: 0.0496 - f1_mean: 0.4266 - f1_std: 0.4151 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1634 - val_f1_mean: 0.1342 - val_f1_std: 0.2441 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8414\n",
      "Epoch 79/300\n",
      "100/100 [==============================] - 69s - loss: 0.0484 - f1_mean: 0.4314 - f1_std: 0.4179 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1700 - val_f1_mean: 0.1134 - val_f1_std: 0.2278 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7867\n",
      "Epoch 80/300\n",
      "100/100 [==============================] - 68s - loss: 0.0488 - f1_mean: 0.4354 - f1_std: 0.4188 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1609 - val_f1_mean: 0.1373 - val_f1_std: 0.2481 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8381\n",
      "Epoch 81/300\n",
      "100/100 [==============================] - 69s - loss: 0.0482 - f1_mean: 0.4371 - f1_std: 0.4188 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1586 - val_f1_mean: 0.1488 - val_f1_std: 0.2552 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8578\n",
      "Epoch 82/300\n",
      "100/100 [==============================] - 68s - loss: 0.0479 - f1_mean: 0.4319 - f1_std: 0.4189 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1584 - val_f1_mean: 0.1430 - val_f1_std: 0.2464 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8293\n",
      "Epoch 83/300\n",
      "100/100 [==============================] - 71s - loss: 0.0483 - f1_mean: 0.4376 - f1_std: 0.4182 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1617 - val_f1_mean: 0.1346 - val_f1_std: 0.2420 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8222\n",
      "Epoch 84/300\n",
      "100/100 [==============================] - 69s - loss: 0.0465 - f1_mean: 0.4345 - f1_std: 0.4207 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1843 - val_f1_mean: 0.1251 - val_f1_std: 0.2370 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8049\n",
      "Epoch 85/300\n",
      "100/100 [==============================] - 69s - loss: 0.0470 - f1_mean: 0.4295 - f1_std: 0.4191 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1716 - val_f1_mean: 0.1056 - val_f1_std: 0.2234 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8259\n",
      "Epoch 86/300\n",
      "100/100 [==============================] - 68s - loss: 0.0470 - f1_mean: 0.4403 - f1_std: 0.4180 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1666 - val_f1_mean: 0.1345 - val_f1_std: 0.2484 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8224\n",
      "Epoch 87/300\n",
      "100/100 [==============================] - 69s - loss: 0.0485 - f1_mean: 0.4298 - f1_std: 0.4175 - f1_min: 0.0000e+00 - f1_max: 0.9991 - val_loss: 0.1752 - val_f1_mean: 0.1476 - val_f1_std: 0.2546 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8642\n",
      "Epoch 88/300\n",
      "100/100 [==============================] - 69s - loss: 0.0472 - f1_mean: 0.4434 - f1_std: 0.4201 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1633 - val_f1_mean: 0.1426 - val_f1_std: 0.2556 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8561\n",
      "Epoch 89/300\n",
      "100/100 [==============================] - 70s - loss: 0.0471 - f1_mean: 0.4287 - f1_std: 0.4189 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1819 - val_f1_mean: 0.1406 - val_f1_std: 0.2508 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8382\n",
      "Epoch 90/300\n",
      "100/100 [==============================] - 68s - loss: 0.0466 - f1_mean: 0.4364 - f1_std: 0.4197 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1674 - val_f1_mean: 0.1272 - val_f1_std: 0.2433 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8520\n",
      "Epoch 91/300\n",
      "100/100 [==============================] - 68s - loss: 0.0458 - f1_mean: 0.4398 - f1_std: 0.4168 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1589 - val_f1_mean: 0.1515 - val_f1_std: 0.2582 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8562\n",
      "Epoch 92/300\n",
      "100/100 [==============================] - 68s - loss: 0.0454 - f1_mean: 0.4446 - f1_std: 0.4195 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1699 - val_f1_mean: 0.1529 - val_f1_std: 0.2623 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8470\n",
      "Epoch 93/300\n",
      "100/100 [==============================] - 69s - loss: 0.0458 - f1_mean: 0.4388 - f1_std: 0.4218 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1720 - val_f1_mean: 0.1552 - val_f1_std: 0.2591 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8421\n",
      "Epoch 94/300\n",
      "100/100 [==============================] - 69s - loss: 0.0464 - f1_mean: 0.4283 - f1_std: 0.4168 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1591 - val_f1_mean: 0.1354 - val_f1_std: 0.2409 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8176\n",
      "Epoch 95/300\n",
      "100/100 [==============================] - 71s - loss: 0.0455 - f1_mean: 0.4293 - f1_std: 0.4180 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1620 - val_f1_mean: 0.1471 - val_f1_std: 0.2578 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8363\n",
      "Epoch 96/300\n",
      "100/100 [==============================] - 68s - loss: 0.0464 - f1_mean: 0.4373 - f1_std: 0.4173 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1776 - val_f1_mean: 0.1307 - val_f1_std: 0.2293 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7401\n",
      "Epoch 97/300\n",
      "100/100 [==============================] - 69s - loss: 0.0462 - f1_mean: 0.4311 - f1_std: 0.4183 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1747 - val_f1_mean: 0.1512 - val_f1_std: 0.2639 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8741\n",
      "Epoch 98/300\n",
      "100/100 [==============================] - 69s - loss: 0.0450 - f1_mean: 0.4401 - f1_std: 0.4182 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1561 - val_f1_mean: 0.1358 - val_f1_std: 0.2489 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8269\n",
      "Epoch 99/300\n",
      "100/100 [==============================] - 68s - loss: 0.0446 - f1_mean: 0.4383 - f1_std: 0.4210 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1694 - val_f1_mean: 0.1226 - val_f1_std: 0.2289 - val_f1_min: 0.0000e+00 - val_f1_max: 0.7594\n",
      "Epoch 100/300\n",
      "100/100 [==============================] - 69s - loss: 0.0454 - f1_mean: 0.4282 - f1_std: 0.4164 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1765 - val_f1_mean: 0.1394 - val_f1_std: 0.2504 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8250\n",
      "Epoch 101/300\n",
      "100/100 [==============================] - 71s - loss: 0.0446 - f1_mean: 0.4409 - f1_std: 0.4190 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1600 - val_f1_mean: 0.1528 - val_f1_std: 0.2564 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8127\n",
      "Epoch 102/300\n",
      "100/100 [==============================] - 69s - loss: 0.0443 - f1_mean: 0.4326 - f1_std: 0.4186 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1680 - val_f1_mean: 0.1460 - val_f1_std: 0.2605 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8790\n",
      "Epoch 103/300\n",
      "100/100 [==============================] - 69s - loss: 0.0441 - f1_mean: 0.4420 - f1_std: 0.4202 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1572 - val_f1_mean: 0.1464 - val_f1_std: 0.2596 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8664\n",
      "Epoch 104/300\n",
      "100/100 [==============================] - 67s - loss: 0.0452 - f1_mean: 0.4354 - f1_std: 0.4165 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1592 - val_f1_mean: 0.1460 - val_f1_std: 0.2571 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8483\n",
      "Epoch 105/300\n",
      "100/100 [==============================] - 67s - loss: 0.0461 - f1_mean: 0.4307 - f1_std: 0.4171 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1674 - val_f1_mean: 0.1455 - val_f1_std: 0.2614 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8717\n",
      "Epoch 106/300\n",
      "100/100 [==============================] - 67s - loss: 0.0414 - f1_mean: 0.4455 - f1_std: 0.4241 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1642 - val_f1_mean: 0.1520 - val_f1_std: 0.2600 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8436\n",
      "Epoch 107/300\n",
      "100/100 [==============================] - 71s - loss: 0.0435 - f1_mean: 0.4393 - f1_std: 0.4198 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1584 - val_f1_mean: 0.1587 - val_f1_std: 0.2672 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8733\n",
      "Epoch 108/300\n",
      "100/100 [==============================] - 69s - loss: 0.0444 - f1_mean: 0.4374 - f1_std: 0.4201 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1711 - val_f1_mean: 0.1489 - val_f1_std: 0.2605 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8545\n",
      "Epoch 109/300\n",
      "100/100 [==============================] - 67s - loss: 0.0433 - f1_mean: 0.4335 - f1_std: 0.4219 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1591 - val_f1_mean: 0.1598 - val_f1_std: 0.2689 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8826\n",
      "Epoch 110/300\n",
      "100/100 [==============================] - 65s - loss: 0.0436 - f1_mean: 0.4393 - f1_std: 0.4191 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1589 - val_f1_mean: 0.1369 - val_f1_std: 0.2477 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8223\n",
      "Epoch 111/300\n",
      "100/100 [==============================] - 68s - loss: 0.0445 - f1_mean: 0.4293 - f1_std: 0.4179 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1606 - val_f1_mean: 0.1477 - val_f1_std: 0.2579 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8662\n",
      "Epoch 112/300\n",
      "100/100 [==============================] - 67s - loss: 0.0426 - f1_mean: 0.4400 - f1_std: 0.4228 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1632 - val_f1_mean: 0.1538 - val_f1_std: 0.2641 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8706\n",
      "Epoch 113/300\n",
      "100/100 [==============================] - 71s - loss: 0.0430 - f1_mean: 0.4378 - f1_std: 0.4192 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1724 - val_f1_mean: 0.1610 - val_f1_std: 0.2664 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8743\n",
      "Epoch 114/300\n",
      "100/100 [==============================] - 67s - loss: 0.0433 - f1_mean: 0.4393 - f1_std: 0.4197 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1624 - val_f1_mean: 0.1598 - val_f1_std: 0.2711 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8955\n",
      "Epoch 115/300\n",
      "100/100 [==============================] - 69s - loss: 0.0443 - f1_mean: 0.4352 - f1_std: 0.4168 - f1_min: 0.0000e+00 - f1_max: 1.0000 - val_loss: 0.1601 - val_f1_mean: 0.1489 - val_f1_std: 0.2566 - val_f1_min: 0.0000e+00 - val_f1_max: 0.8471\n",
      "Epoch 116/300\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0418 - f1_mean: 0.4345 - f1_std: 0.4240 - f1_min: 0.0000e+00 - f1_max: 0.9986"
     ]
    }
   ],
   "source": [
    "model = ImprovedModel(parameter, use_dropout=use_dropout)\n",
    "model.build_model()\n",
    "model.compile_model()\n",
    "model.set_generators(training_generator, validation_generator)\n",
    "epoch_history = model.learn()\n",
    "proba_predictions = model.predict(predict_generator)\n",
    "#model.save(\"improved_model.h5\")\n",
    "improved_proba_predictions = pd.DataFrame(proba_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
